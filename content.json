{"meta":{"title":"OreChou's Batcave","subtitle":"Coding & Reading & Playing & Listening","description":"Coding & Reading & Playing & Listening","author":"OreChou","url":"https://orechou.github.io","root":"/"},"pages":[{"title":"Books Of My Life.","date":"2022-08-20T05:46:16.931Z","updated":"2022-04-08T16:13:27.313Z","comments":true,"path":"book/index.html","permalink":"https://orechou.github.io/book/index.html","excerpt":"","text":"技术书籍《数据密集型应用系统设计》豆瓣《Effective Java中文版（第3版）》豆瓣 金融书籍《区块链实战》该书比较全面的介绍区块链基本原理、历史、发展和应用，是一本很好的概述性的书籍。对于区块链的技术细节和一些应用架构有所涉及，但不会深入。想要了解区块链的读者可以各取所需，到对应的章节了解想要了解的内容。 非技术书籍《读水浒：人性的十三种刻度》一本闲书，用了很快的时间读完。作者对水浒里面特殊的人物做了一些解读，又没有过渡解读。少年的时候读水浒，觉得水浒好玩。现如今长大了些，再读水浒，有了一些惊心动魄。人在成长，适应、妥协、接受着这个世界。"},{"title":"Gallery Of My Life.","date":"2022-08-20T05:46:17.262Z","updated":"2022-03-27T11:17:10.268Z","comments":true,"path":"gallery/index.html","permalink":"https://orechou.github.io/gallery/index.html","excerpt":"","text":"西门🐱：Pikachu"},{"title":"About me.","date":"2022-11-01T14:36:46.933Z","updated":"2022-11-01T14:36:46.933Z","comments":true,"path":"about/index.html","permalink":"https://orechou.github.io/about/index.html","excerpt":"","text":"做了很多事情，也什么事情没做。 Location0 ~ 25 years，重庆25 ~ Now, 深圳 Education2012 ~ 2016 重庆大学|计算机学院|计算机科学与技术|本科2016 ~ 2019 重庆大学|计算机学院|计算机科学与技术|硕士CCF-C PapersSpatially fine-grained air quality prediction based on DBU-LSTMA Novel Approach for Air Quality Inference and Prediction Based on DBU-LSTMDeep Spatial-Temporal Fusion Network for Fine-Grained Air Quality Prediction Career2019.7 ~ 2021.6 维沃移动通信(深圳)有限公司|影像系统集成开发部|应用工程师负责影像相关的软件重构；预览平滑 VSYNC 优化。创新奖、优秀奖。 2021.6 ~ 至今 牛客|企业服务部|后端工程师负责牛客题库中台。负责牛客竞赛。 MeaningCoding &amp; Reading &amp; Playing &amp; Listening希望能够体验、记录更多的生活。"},{"title":"Games Of My Life.","date":"2022-08-20T05:46:16.931Z","updated":"2022-05-13T16:25:55.888Z","comments":true,"path":"game/index.html","permalink":"https://orechou.github.io/game/index.html","excerpt":"","text":"喜欢的游戏【1】Batman: Arkham Knight很喜欢在暴雨、黑夜中的哥谭市，扮演这位黑暗骑士的感觉。孤独、隐忍与信念。 【2】Middle-earth: Shadow of Mordor原创的一个刚铎游侠为守卫世界最后成为戒灵的故事。复仇、纠结与牺牲。游戏的结尾曲感人。 【3】Red Dead Redemption 2Read Dead Redemption 2 可以无限吹，18 年最佳游戏落败给战神 4 我是没想到的。"},{"title":"关于DP","date":"2022-08-20T05:46:17.261Z","updated":"2016-03-02T10:48:31.000Z","comments":true,"path":"back_posts/old_posts/DP.html","permalink":"https://orechou.github.io/back_posts/old_posts/DP.html","excerpt":"","text":"QuikSork今天遇到一道关于DP的题，复习一下DP 解题步骤1.刻画一个最优解得特征结构2.递归地定义最优解的值3.计算最优解的值，通常采用自底向上的方法4.利用计算出的信息构造一个最优解 概念最优子结构性质：问题的最优解由相关子问题的最优解组合而成，而这些子问题可以独立求解对于任何子问题，直至它依赖的所有子问题均已求解完成才会求解它 题目Problem Description：Given a sequence a[1],a[2],a[3]......a[n], your job is to calculate the max sum of a sub-sequence. For example, given (6,-1,5,4,-7), the max sum in this sequence is 6 + (-1) + 5 + 4 = 14.Input:The first line of the input contains an integer T(1&lt;=T&lt;=20) which means the number of test cases. Then T lines follow, each line starts with a number N(1&lt;=N&lt;=100000), then N integers followed(all the integers are between -1000 and 1000).Output:For each test case, you should output two lines. The first line is &quot;Case #:&quot;, # means the number of the test case. The second line contains three integers, the Max Sum in the sequence, the start position of the sub-sequence, the end position of the sub-sequence. If there are more than one result, output the first one. Output a blank line between two cases.Sample Input:2 5 6 -1 5 4 -7 7 0 6 -1 1 -6 7 -5Sample Output:Case 1: 14 1 4Case 2: 7 1 6 代码#include &lt;stdio.h&gt; #define maxsize 1000000 int main() &#123; int seq[maxsize]; int T,N; scanf(&quot;%d&quot;, &amp;T); for (int i = 0; i &lt; T; ++i) &#123; scanf(&quot;%d&quot;, &amp;N); for (int k = 0; k &lt; N; ++k) &#123; scanf(&quot;%d&quot;, &amp;seq[k]); &#125; int max = 0, temp = 0, start = 0, end = 0, pos = 0; for (int j = 0; j &lt; N; ++j) &#123; /* code */ if (temp + seq[j] &lt; seq[j]) &#123; /* code */ pos = j; temp = seq[j]; &#125; else &#123; temp += seq[j]; &#125; if (temp &gt; max) &#123; max = temp; start = pos; end = j; &#125; &#125; printf(&quot;Case %d:\\n&quot;, i+1); printf(&quot;%d %d %d&quot;, max , start+1, end+1); if (i == T-1) &#123; printf(&quot;\\n&quot;); &#125; else &#123; printf(&quot;\\n\\n&quot;); &#125; &#125; &#125;"},{"title":"Build this Blog","date":"2022-08-20T05:46:16.860Z","updated":"2016-01-13T09:19:49.000Z","comments":true,"path":"back_posts/old_posts/Blog.html","permalink":"https://orechou.github.io/back_posts/old_posts/Blog.html","excerpt":"","text":"date: 2016-01-13 16:44:51 简简单单记录几句吧。 Quick Start第一点其实现在搭这个Blog，有一半是装逼。学技术才写博客，我现在算本末倒置。搭建Blog使用了Github+Hexo，搭了半天。Hexo是基于Node.js的静态博客框架。搭很简单，尴尬的是Node.js被墙，我在mac环境下用brew一直下载失败。网上有解决方法。当然教程有很多。自己搭博客的好处：自定义你的界面等等。最重要的是：你只需在电脑里用Makedown把post写好，发布只需要在命令行里敲两句“hexo generate”，“hexo deploy” 第二点这半年来考研，没学专业知识，也没编程。我还算不上解编程，菜鸟刚飞。人有兴趣，做事情才会有动力，我还有兴趣。 第三点给我有感受的是，人的工作、学习效率与使用的工具有很大关系，别让工具拖住人的步伐。但对我来说，我的脑子拖着我步伐，学习是件很吃力的事情。新事物层出不穷，跟不上的，注定淘汰。 That’s All"},{"title":"Android Notes","date":"2022-08-20T05:46:16.860Z","updated":"2016-01-25T12:52:35.000Z","comments":true,"path":"back_posts/old_posts/Android_Notes.html","permalink":"https://orechou.github.io/back_posts/old_posts/Android_Notes.html","excerpt":"","text":"Quick Start1、在程序中实现事件监听器的4种形式内部类（优点：可以在当前类中服用该监听器；可以自由访问外部类的所有界面组件）外部类（优点：确实需要被多个GUI界面共享，而且主要是完成某种业务逻辑的实现）（But：实际上不推荐将业务逻辑实现写在事件监听器中，包含业务逻辑的事件监听器将导致程序的显示逻辑和业务逻辑耦合。通常考虑使用业务逻辑组件来定义业务逻辑功能，再让事件监听器来调用业务逻辑组件的业务逻辑方法。）Activity本身作为事件监听器（缺点：程序结构混乱；诡异）匿名内部类（事件处理器没有复用价值。But：事实上这是目前使用最广泛的形式）直接绑定到标签（函数是public，参数传入一个View） 2、基于监听的事件处理比基于回调的事件处理的优势（1）基于监听的时间处理模型分工更明确，事件源、事件监听器有两个类分开实现，具有更好的可维护性。（2）Android的事件处理机制保证基于监听的事件监听器会被优先触发。 3、Activity的四种加载模式Android采用Task来管理多个Activity。可以把Task理解成Activity栈，Task以栈的形式来管理Activity。（1）standard模式：Android总会为目标Activity创建一个新的实例，并将该Activity添加到当前Task栈中，这种模式不会启动新的Task，新Activity将被添加到原有的Task中。（2）singleTop模式：当将要启动的目标Activity已经位于Task栈顶时，系统不会重新创建目标Activity的实例，而是直接复用已有的Activity实例。（3）singleTask模式：要启动的目标Activity已经存在、但没有位于Task栈顶，系统将会把位于该Activity上面的所有Activity移出Task栈，从而使得目标Activity转入栈顶。（4）singleInstance模式：启动的目标Activity不存在，系统先创建一个新的Task，再创建目标Activity实例，并将它加入新的Task栈顶。如果目标Activity已经存在，无论他位于哪个应用程序中、位于哪个Task中，系统都会把该Activity所在的Task转到前台，从而使该Activity显示出来。"},{"title":"ORM & JPA  & Spring Data JPA 之间的关系","date":"2022-08-20T05:46:15.406Z","updated":"2018-06-12T12:54:30.000Z","comments":true,"path":"back_posts/old_posts/JPA & ORM & Spring Data JPA 之间的关系.html","permalink":"https://orechou.github.io/back_posts/old_posts/JPA%20&%20ORM%20&%20Spring%20Data%20JPA%20%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.html","excerpt":"这篇文章主要梳理一下 ORM &amp; JPA &amp; Spring Data JPA 三者之间的关系，后面会写一篇现在主流 ORM 框架（Hibernate、Mybatis）的文章。","text":"这篇文章主要梳理一下 ORM &amp; JPA &amp; Spring Data JPA 三者之间的关系，后面会写一篇现在主流 ORM 框架（Hibernate、Mybatis）的文章。 ORMWhat对象关系映射( Object Relational Mapping，简称 ORM ），是一种程序技术，用于实现面向对象编程语言里不同类型系统的数据之间的转换。 Why在原来 Java 对数据库的访问是通过 JDBC（Java Database Connectivity），JDBC 提供了一组 Java API 来访问数据库。使用 JDBC 开发应用程序的优缺点如下： JDBC 的优点 JDBC 的缺点 干净整洁的 SQL 处理 大项目中使用很复杂 大数据下有良好的性能 很大的编程成本 对于小应用非常好 没有封装 易学的简易语法 难以实现 MVC 的概念 针对 JDBC 存在的缺点，一些厂商开发了 ORM 框架来弥补。比如：Hibernate、iBatis（MyBatis）、openJDK等。使用 ORM 的优点如下： 序号 优点 1 一个 API 来在持久类的对象上实现基本的 CRUD 操作 2 一个语言或 API 来指定引用类和属性的查询 3 一个可配置的服务用来指定映射元数据 4 一个技术和事务对象交互来执行 dirty checking, lazy association fetching 和其它优化的功能 但是现在也出现了一个问题， ORM 框架这么多，大家各自搞自己的，没有一个统一的规范。所以 JPA 就出现了。 JPAWhatJava Persistence API（JPA）即Java持久层API，是一种Java应用程序编程接口规范，它描述了在使用Java平台标准版和Java平台企业版的应用程序中管理关系数据。 持久层是一个包括了以下三个方面的技术： 定义在 javax.persistence 包下的API本身：用来操作实体对象，执行CRUD操作，框架在后台替代我们完成所有的事情，开发者从繁琐的JDBC和SQL代码中解脱出来。 Java持久层查询语言（JPQL）：这是持久化操作中很重要的一个方面，通过面向对象而非面向数据库的查询语言查询数据，避免程序的SQL语句紧密耦合。 对象关系元数据：JPA支持XML和JDK5.0注解两种元数据的形式，元数据描述对象和表之间的映射关系，框架据此将实体对象持久化到数据库表中 WhySun引入新的JPA ORM规范出于两个原因：其一，简化现有Java EE和Java SE应用开发工作；其二，Sun希望整合ORM技术，实现天下归一。开发者面向 JPA 规范的接口，但底层的 JPA 实现可以任意切换：觉得 Hibernate 好的，可以选择Hibernate JPA 实现；觉得 TopLink 好的，可以选择 TopLink JPA 实现。 JPA 只是提供了一个接口规范，在实际的工程应用中，光靠 JPA 做不了实际的事情。 Spring Data JPAWhat官方给出的定义：Spring Data JPA 可以轻松实现基于JPA的存储库。该模块处理对基于JPA的数据访问层的增强支持。它使构建使用数据访问技术的Spring应用程序变得更加容易。 Why在一段时间内实现应用程序的数据访问层很麻烦。需要编写太多的样板代码才能执行简单查询以及执行分页和审计。 Spring Data JPA旨在通过减少实际需要的数量来显着改进数据访问层的实现。作为开发人员，您需要编写存储库接口（包括自定义查找程序方法），Spring将自动提供实现。 这样就清楚了，Spring Data JPA 增强了基于 JPA 的数据访问层的支持，并且对于底层的 ORM 提供了自动的实现（Spring Data JPA 底层 ORM 就是通过 Hibernate 实现的）。 SummaryORM 框架提供了面向对象方式，方便了开发人员对数据库的操作。各大厂商各自的 ORM 设计发展需要一个规范统一起来，是 JPA 出现的主要原因。在 Java Web 领域的巨头 Spring 推出的 Spring Data JPA 即增强了 JPA，又屏蔽掉了底层 ORM 的实现。极大程度地方便了开发者。最后可以通过如下的图描述三者之间的关系："},{"title":"JavaScript 严格模式","date":"2022-08-20T05:46:15.407Z","updated":"2018-06-12T12:54:28.000Z","comments":true,"path":"back_posts/old_posts/JS 严格模式.html","permalink":"https://orechou.github.io/back_posts/old_posts/JS%20%E4%B8%A5%E6%A0%BC%E6%A8%A1%E5%BC%8F.html","excerpt":"概述ECMAScript 严格模式是采用具有限制性 JavaScript 变体的一种方式。其目的主要有以下几点： 消除 JavaScript 语法的一些不合理、严谨的地方，减少一些怪异行为 消除代码运行的一些不安全之处，保证代码运行的安全 提高编译器效率，增加运行速度 为未来版本的 JavaScript 做好铺垫","text":"概述ECMAScript 严格模式是采用具有限制性 JavaScript 变体的一种方式。其目的主要有以下几点： 消除 JavaScript 语法的一些不合理、严谨的地方，减少一些怪异行为 消除代码运行的一些不安全之处，保证代码运行的安全 提高编译器效率，增加运行速度 为未来版本的 JavaScript 做好铺垫 使用方式对于脚本将 use strict 放在整个脚本的第一行（必须放置在第一行，否则无效），整个脚本以严格模式运行。 1234&lt;script&gt; &quot;use strict&quot; console.log(&quot;This is in strict mode.&quot;)&lt;/script&gt; 上面的写法有缺点，当有多个脚本文件的时候，不利于文件的合并。变通的做法是将整个脚本文件放在立即执行的匿名函数中 1234(function() &#123; &#x27;use strict&#x27; // follow codes&#125;)(); 将 use strict 放在函数的第一行，整个函数以严格模式运行 1234function strict() &#123; &#x27;use strict&#x27; console.log(&#x27;This is in strict mode.&#x27;)&#125; 带来的改变全局变量必须显示声明123&#x27;use strict&#x27;v = 1 // 报错。v 未声明for (i = 0; i &lt; 10; i++) &#123;&#125; // 报错。i 未声明 禁止使用 with 语句1234567&#x27;use strict&#x27;var v = 1// 严格模式限制了一些动态绑定// 在这里 with 语句无法在编译时确定属性归属于哪个对象with (o) &#123; // 语法错误 v = 2&#125; eval 作用域正常模式下有两种作用域（ scope ）：全局作用域、函数作用域。严格模式下有了第三种作用域， eval 作用域 12345&#x27;use strict&#x27;var v = 1// eval 创建了自身的作用域，它所生成的变量只用于 eval 内部console.log(eval(&#x27;var v = 2; v&#x27;)) // 输出 2console.log(v) // 输出 1 禁止 this 指向全局对象1234567function f() &#123; return !this // false, this 为全局对象，不为 null 和 undefined，所以返回 false&#125;function f() &#123; &#x27;use strict&#x27; return !this // true, this 不指向全局对象，为 undefined&#125; 禁止在函数内部遍历调用栈123456function f() &#123; &#x27;use strict&#x27; // properties may not be accessed on strict mode functions or the arguments objects for calls to them f.caller // 报错 f.arguments // 报错&#125; 删除变量或属性的限制12345678910111213141516&#x27;use strict&#x27;var a = 1delete a // 报错，禁止删除变量var obj = Object.create(null, &#123; &#x27;a&#x27;: &#123; value: 1, configurable: true &#125;, &#x27;b&#x27;: &#123; value: 2, configurable: false &#125;&#125;)// 只有 configurable 为 true 的对象属性才能被删除delete obj.a // 成功delete obj.b // 失败 显示报错正常模式下有些行为会失败，不会报错；严格模式下将报错，情况如下： 对一个对象的只读属性赋值 对一个只使用 getter 方法读取的属性赋值 对禁止扩展的对象添加新属性 删除一个不可删除的属性 重名的错误情况如下： 对象不能有重名的属性 函数不能有重名的参数 123456&#x27;use strict&#x27;var obj = &#123; p: 1, p: 2 &#125;function f(a, a, b) &#123;&#125; 禁止八进制表示法123&quot;use strict&quot;;// 正常模式下，整数的第一位如果是0，表示这是八进制数var a = 0100; // 语法错误 arguments 对象的限制arguments 对象是所有非箭头函数中都可用的局部变量 限制如下： 不允许对 arguments 赋值 arguments 不再追踪参数变化 禁止使用 arguments.callee 12345678910111213141516171819202122232425262728&quot;use strict&quot;;// 不允许对 arguments 赋值arguments++; // 语法错误var obj = &#123; set p(arguments) &#123; &#125; &#125;; // 语法错误try &#123; &#125; catch (arguments) &#123; &#125; // 语法错误function arguments() &#123; &#125; // 语法错误var f = new Function(&quot;arguments&quot;, &quot;&#x27;use strict&#x27;; return 17;&quot;); // 语法错误// arguments 不再追踪参数变化function f(a) &#123; a = 2; return [a, arguments[0]];&#125;f(1); // 正常模式为[2,2]function f(a) &#123; &quot;use strict&quot;; a = 2; return [a, arguments[0]];&#125;f(1); // 严格模式为[2,1]// 禁止使用 arguments.callee&quot;use strict&quot;;var f = function() &#123; // argumens.callee 指向当前执行的函数 return arguments.callee; &#125;;f(); // 报错 函数不能声明在非函数的代码块中1234&quot;use strict&quot;;if (true) &#123; function f() &#123; &#125; // 语法错误&#125; 新增加的保留字implements, interface, let, package, private, protected, public, static, yield 保留字无法作为变量名"},{"title":"July-9-2016","date":"2022-08-20T05:46:16.860Z","updated":"2016-07-09T13:26:52.000Z","comments":true,"path":"back_posts/old_posts/July-9-2016.html","permalink":"https://orechou.github.io/back_posts/old_posts/July-9-2016.html","excerpt":"","text":"距离上篇博文已经过去了两个月，寻思再不更新些什么，这个Blog的意义也就不在了。今日不更新，以后也不会更新了。这两个月，毕业、约会、做项目，另外还去了一趟澳门。很感谢我的导师——葛老师带给的这次机会，去见识了一下澳门的赌场和一些优秀的计算机idea。 关于项目从五月份开始做的项目是为重庆市城市建设发展有限公司做的一个项目管理web软件。从最开始接触tern框架开始，再慢慢接触业务逻辑，到后面对项目的思考。这一个多月还是挺有收获，主要有以下几点：1、参与了完整的项目开发；2、锻炼了前端与后端的编程能力（从页面设计到服务器API的编写）；3、框架底层代码的阅读（自己调试bug的时候，会深入去看一看框架底层的代码。例如：数据库的再次封装的代码；线程池分配的代码）。昨天带着项目简陋版本去业主方展示，对于业主方的需求，目前的web app还有很多东西要去实现。 关于毕业毕业只有几句话说。天之涯，地之角，知交半零落。一壶浊酒尽馀欢，今宵别梦寒。祝各位前程似锦，珍重。 关于澳门此次去澳门是参加泛珠三角计算机作品赛。认识了几位科技学院的同学，见识了赌场。葛老师还请吃了猪扒包和澳门蛋挞。😄"},{"title":"Java并发容器ConcurrentLinkedQueue源码解析","date":"2022-08-20T05:46:17.261Z","updated":"2018-06-12T12:56:38.000Z","comments":true,"path":"back_posts/old_posts/Java 并发容器ConcurrentLinkedQueue 源码解析.html","permalink":"https://orechou.github.io/back_posts/old_posts/Java%20%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8ConcurrentLinkedQueue%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html","excerpt":"Java提供了多种并发队列，具体如下： 无锁非阻塞队列：ConcurrentLinkedQueue &amp; ConcurrentLinkedDeque 普通阻塞队列：ArrayBlockingQueue &amp; LinkedBlockingQuene &amp; LinkedBlockingDeque","text":"Java提供了多种并发队列，具体如下： 无锁非阻塞队列：ConcurrentLinkedQueue &amp; ConcurrentLinkedDeque 普通阻塞队列：ArrayBlockingQueue &amp; LinkedBlockingQuene &amp; LinkedBlockingDeque 1、ConcurrentLinkedQueueConcurrentLinkedQueue和ConcurrentLinkedDeque是无锁非阻塞的，其实现方式是通过循环CAS（wait-free算法）。内部的数据结构是链表，都没有大小限制，容量是无限的。其 size() 遍历整个队列统计个数，不是一个O(1)时间的方法，并且返回的值不能准确的反应队列的实际大小。 源码分析head &amp; tail 节点head 和 tail 并不是一定指向整个队列的首部，和尾部。其满足一些规则：不变性和可变性。 123456789101112131415161718192021222324252627/** * A node from which the first live (non-deleted) node (if any) * can be reached in O(1) time. * Invariants（不变性）: * - all live nodes are reachable from head via succ() （所有的节点都可以通过 head 使用 succ() 方法取得） * - head != null （head 节点不为 null） * - (tmp = head).next != tmp || tmp != head （其实就是 head.next != head） * Non-invariants: * - head.item may or may not be null. （head 节点的值可能为null） * - it is permitted for tail to lag behind head, that is, for tail * to not be reachable from head! （允许 tail 滞后于 head, 也就是调用 succ() 方法, 从 head 不可达tail） */ private transient volatile Node&lt;E&gt; head; /** * A node from which the last node on list (that is, the unique * node with node.next == null) can be reached in O(1) time. * Invariants: * - the last node is always reachable from tail via succ() （tail 节点通过succ()方法一定到达队列中的最后一个节点(node.next = null)） * - tail != null * Non-invariants: * - tail.item may or may not be null. （tail 节点的值可能为null） * - it is permitted for tail to lag behind head, that is, for tail * to not be reachable from head! （允许 tail 滞后于 head, 也就是调用 succ() 方法, 从 head 不可达tail） * - tail.next may or may not be self-pointing to tail. （tail.next 可能指向 tail） */ private transient volatile Node&lt;E&gt; tail; 入队列offer元素入队的主要的工作： 将入队节点设置成当前队列尾节点的下一个节点 更新 tail 节点（tail节点不总是为队尾节点，减少了CAS更新 tail 节点的次数，提高入队效率）： 若 tail 节点的next节点不为空，则将入队节点设置成 tail 节点 若 tail 节点的next节点为空，则将入队节点设置成 tail 的next节点 123456789101112131415161718192021222324252627282930313233public boolean offer(E e) &#123; checkNotNull(e); // 根据传入的值构造节点 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); // 通过 tail 节点找到队列的尾节点 for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; // 定位p为队列的尾节点 if (q == null) &#123; // 使用cas将新节点加入到队尾 // 比较p的next是否为null，为null才会设置成功。这里实现了同步，防止了别人已经修改了队尾，而设置了错误的位置 // if如果失败，则会循环再次执行 if (p.casNext(null, newNode)) &#123; // 插入成功之后，判断队尾节节点的前一个节点是否是tail节点，若不是，则将tail节点指向队尾节点 if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; &#125; // Lost CAS race to another thread; re-read next &#125; else if (p == q) // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. // 这里的情况是 p == p.next，并且t为tail节点。表示这个队列刚初始化，正在准备添加节点，所以返回head节点 p = (t != (t = tail)) ? t : head; else // Check for tail updates after two hops. // 重新赋值p结点，让p成为尾节点 p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 出队poll元素出队的主要工作： 根据 head 节点找到队列首个不为空的节点，并CAS让该节点出队 更新 head 节点（head 节点不总是为队首节点，减少了CAS更新 head 节点的次数，提高入队效率）： 若 head 节点有元素，不会更新head节点 若 head 节点没有元素，将 head 指向队列的首节点 123456789101112131415161718192021222324252627282930public E poll() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; // 若头节点的值不为空，并且cas设置头节点值为null成功 if (item != null &amp;&amp; p.casItem(item, null)) &#123; // Successful CAS is the linearization point // for item to be removed from this queue. // 若当前节点不是 head 节点 if (p != h) // hop two nodes at a time // 若当前节点的下一个节点不为空，则将 head 节点指向当前节点的下一个节点，否者指向当前节点 updateHead(h, ((q = p.next) != null) ? q : p); // 返回出队的值 return item; &#125; // 若当前节点的下一个节点为空，则该队列已空，重新设置 head 节点，并返回null else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; // 该情况为，p是已经删除的 head 节点，继续循环 else if (p == q) continue restartFromHead; else // 重新赋值p节点 p = q; &#125; &#125; &#125; 使用场景因为其采用的是无锁的方式，所以能够满足高并发情况下的需求，其性能优于BlockingQueue。缺点在于其size()方法为一个O(n)时间复杂度的方法，并且返回的值并不是一个准确的值，所以在实际应用中，无法有效获取队列中节点的个数。"},{"title":"May-8-2016","date":"2022-08-20T05:46:17.044Z","updated":"2016-05-08T15:28:46.000Z","comments":true,"path":"back_posts/old_posts/May-8-2016.html","permalink":"https://orechou.github.io/back_posts/old_posts/May-8-2016.html","excerpt":"","text":"今天晚上刚把毕业论文写好发给导师。这周已经开始跟导师做项目了。老师同学做的一个JavaWeb开发框架——tern，该框架已经在GitHub上开源。我们使用这个JavaWeb框架开发WebApp。一期项目是做一个面向企业用户的项目管理系统CPMS。熟悉这个框架有一周多的时间，谈一谈自己的感受：1、相比于SSH、亦或是SpringMVC，tern很轻量级。在配置工程上，几乎不需要做什么配置。2、对于基本的数据库支持很完善，数据库查询功能齐全。可能不足的地方在于web性能问题，该框架不适用于开发大型的用户量大的网站。3、具有很大的复用性，该框架很适合于开发企业的管理应用系统。目前开源的Tern分为三个部分：tern、tern-ui、tern-iap。根据作者乔老师所说，tern与tern-ui已经接近完善、不会再有改变，tern-iap仍在开发完善之中。大三学习web开发的时候没有学习前端知识，目前做项目的过程中，要对前端知识有一定的掌握。所以这周也在学习前端，web前端技术：HTML、CSS、JavaScript，我主要深入学习JavaScript。JavaScript的学习意义不止在于制作前端页面，更在于前端业务逻辑与前端数据与UI的交互。剩下的HTML和CSS，在目前的学习中，就是用到的时候Google一下就ok。我们在项目中前端用到的技术有：Bootstrap、JQuery（选择器省去我好多工作量）和FreeMarker。FreeMarker用于使用Java语言定义前端UI模板、指令标签，并且还有将数据库中所得数据与模板结合展示的作用。学习三门技术的最好途径就是上官网，看官网给的示例，然后根据项目一步一步实践。 ok。第一篇周记就这些。"},{"title":"Linux 命令基础","date":"2022-08-20T05:46:17.044Z","updated":"2018-09-15T07:19:41.000Z","comments":true,"path":"back_posts/old_posts/Linux 命令基础.html","permalink":"https://orechou.github.io/back_posts/old_posts/Linux%20%E5%91%BD%E4%BB%A4%E5%9F%BA%E7%A1%80.html","excerpt":"Linux 基础命令是任何一个开发人员应该掌握的基本技能。大多数项目的生产部署环境都会选择 Linux，所以在项目运维和故障排除的时候，Linux 命令能够起到关键的作用。","text":"Linux 基础命令是任何一个开发人员应该掌握的基本技能。大多数项目的生产部署环境都会选择 Linux，所以在项目运维和故障排除的时候，Linux 命令能够起到关键的作用。 获取文件内容基本指令 命令 描述 备注 cat 打印文件内容 -n: 打印出行号；-b: 不打印出行号 tac 是 cat 的反向操作，从最后一行开始打印 more 一页一页查看文件内容，比较适合大文件的查看 less 和 more 类似，但是多了一个向前翻页的功能 head 打印文件前几行 head -n filename tail head 的反向操作，打印后几行 od 以字符或者十六进制的形式显示二进制文件 指令 &amp; 文件搜索基本指令 命令 描述 备注 which 指令搜索 -a: 列出所有指令，而不是列出一个 whereis 文件搜索。速度非常快， 只搜索几个特定的目录 locate 文件搜索。可以用关键字或者正则表达式进行搜索 find 文件搜索。可以使用文件的属性和权限进行搜索 find 指令详解指令的基础用法如下： 12# find [basedir] [option] // basedir 不指定则为当前目录，及其子目录find . -name &quot;shadow*&quot; 按修改时间搜索的参数1234-mtime n ：列出在 n 天前的那一天修改过内容的文件-mtime +n ：列出在 n 天之前 (不含 n 天本身) 修改过内容的文件-mtime -n ：列出在 n 天之内 (含 n 天本身) 修改过内容的文件-newer file ： 列出比 file 更新的文件 按文件所属搜索的参数123456-uid n-gid n-user name-group name-nouser ：搜索拥有者不存在于 /etc/passwd 的文件-nogroup：搜索所属群组不存在于 /etc/group 的文件 按文件属性搜索的参数123456-name filename-size [+-]SIZE：搜寻比 SIZE 还要大 (+) 或小 (-) 的文件。这个 SIZE 的规格有：c: 代表 byte，k: 代表 1024bytes。所以，要找比 50KB 还要大的文件，就是 -size +50k-type TYPE-perm mode ：搜索权限等于 mode 的文件-perm -mode ：搜索权限包含 mode 的文件-perm /mode ：搜索权限包含任一 mode 的文件 正则表达式基本指令 指令 描述 备注 grep 使用正则表示式进行全局查找并打印 printf 用于格式化输出 在给 printf 传数据时需要使用 $( ) 形式 awk 每次处理一行，处理的最小单位是字段 grep 详解指令 grep （globally search a regular expression and print）的基本用法如下： 123456$ grep [-acinv] [--color=auto] 搜寻字符串 filename-c ： 统计个数-i ： 忽略大小写-n ： 输出行号-v ： 反向选择，也就是显示出没有 搜寻字符串 内容的那一行--color=auto ：找到的关键字加颜色显示 示例如下： 1234567# test.txt 的内容如下hello world123aaaaa$ gerp -n &#x27;world&#x27; test.txt1:hello world awk 详解指令 awk 的基本用法如下： 1$ awk &#x27;&#123;[pattern] action&#125;&#x27; &#123;filenames&#125; 每次处理输入流中的一行，然后根据设置的条件匹配相应的字段，然后做处理。 awk 变量： 变量名 代表意义 NF 每一行拥有的字段总数 NR 目前所处理的是第几行数据 FS 目前的分隔字符，默认是空格键 示例如下： 123456789101112131415# 取出登录用户的用户名和 IP$ last -n 5dmtsai pts/0 192.168.1.100 Tue Jul 14 17:32 still logged indmtsai pts/0 192.168.1.100 Thu Jul 9 23:36 - 02:58 (03:22)dmtsai pts/0 192.168.1.100 Thu Jul 9 17:23 - 23:36 (06:12)dmtsai pts/0 192.168.1.100 Thu Jul 9 08:02 - 08:17 (00:14)dmtsai tty1 Fri May 29 11:55 - 12:11 (00:15)# $1, $3 代表了第几个段，每行根据空格和tab来划分段$ last -n 5 | awk &#x27;&#123;print $1 &quot;\\t lines: &quot; NR &quot;\\t columns: &quot; NF&#125;&#x27;dmtsai lines: 1 columns: 10dmtsai lines: 2 columns: 10dmtsai lines: 3 columns: 10dmtsai lines: 4 columns: 10dmtsai lines: 5 columns: 9 Pipeline 指令管线指令 | 将一个命令的标准输出作为另一个命令的标准输入，在数据需要经过多个步骤的处理之后才能得到我们想要的内容时就可以使用 Pipeline。 下面的指令通常与管线指令搭配使用，将其它指令的输出作为指令的输入。 字符转换基本指令 指令 描述 备注 tr 删除一行中的字符，或者对字符进行替换 col 将 tab 字符转为空格字符 expand 将 tab 转换一定数量的空格，默认是 8 个 join 将有相同数据的那一行合并在一起 paste 直接将两行粘贴在一起 split 将一个文件划分成多个文件 cut 指令cut 对数据进行一行一行的切分，用法如下： 1234$ cut -d-d ：分隔符-f ：经过 -d 分隔后，使用 -f n 取出第 n 个区间-c ：以字符为单位取出区间 sort 指令sort 对数据进行排序，用法如下： 123456789$ sort [options] [file &amp; stdin] -f ：忽略大小写-b ：忽略最前面的空格-M ：以月份的名字来排序，例如 JAN，DEC-n ：使用数字-r ：反向排序-u ：相当于 unique，重复的内容只出现一次-t ：分隔符，默认为 tab-k ：指定排序的区间 uniq 指令uniq 只将重复的数据读取一次，用法如下： 123$ uniq [-ic]-i ：忽略大小写-c ：进行计数 tee 指令输出重定向会将输出内容重定向到文件中，而 tee 不仅能够完成这个功能，还能保留屏幕上的输出。也就是说，使用 tee 指令，一个输出会同时传送到文件和屏幕上。 进程管理基本指令 指令 描述 备注 ps 查看某个时间点的进程信息 -l: 查看自己的进程；-aux: 查看系统进程 top 实时显示进程信息 pstree 查看进程树 netstat 查看占用端口的进程 查看特定端口的进程：netstat -anp | grep port 进程状态在 Linux 中进程一共有 6 种状态，如下所示： 状态 说明 R running or runnable (on run queue) D uninterruptible sleep (usually I/O) S interruptible sleep (waiting for an event to complete) Z zombie (terminated but not reaped by its parent) T stopped (either by a job control signal or because it is being traced) X dead (should never be seen) 常用指令查看程序对应的进程号ps -ef|grep 进程名 查看进程占用的端口号netstat -anp|grep pid 查看端口号所对应的进程号lsof -i:端口号 数据流重定向重定向指的是使用文件代替标准输入、标准输出和标准错误输出。 描述 代码 运算符 标准输入 (stdin) 0 &lt; 或 &lt;&lt; 标准输出 (stdout) 1 &gt; 或 &gt;&gt; 标准错误输出 (stderr) 2 2&gt; 或 2&gt;&gt; 注意： 一个箭头的表示以覆盖的方式重定向，两个箭头的表示以追加的方式重定向。 可以将不需要的标准输出以及标准错误输出重定向到 /dev/null，相当于扔进垃圾箱。 如果需要将标准输出以及标准错误输出同时重定向到一个文件，需要将某个输出转换为另一个输出。例如 2&gt;&amp;1 表示将标准错误输出转换为标准输出。 指令搜索的顺序在类 Unix 操作系统中，按照如下的顺序搜索指令： 以绝对或相对路径来执行指令，例如：/bin/ls 或者 ./ls 由别名找到该指令来执行 由 Bash 内建的指令来执行 按 PATH 变量指定的搜索路径的顺序来找到指令来执行"},{"title":"Tensor Decomposition in TensorLy","date":"2022-08-20T05:46:15.208Z","updated":"2018-04-01T12:18:57.000Z","comments":true,"path":"back_posts/old_posts/TensorLy_In_Action.html","permalink":"https://orechou.github.io/back_posts/old_posts/TensorLy_In_Action.html","excerpt":"最近因论文需要而学习张量和张量分解。看了诸多论文之后，想使用一些开源工具来实际操作一下张量。就选择了TensorLy，选择它的原因主要因为它文档写得详细…","text":"最近因论文需要而学习张量和张量分解。看了诸多论文之后，想使用一些开源工具来实际操作一下张量。就选择了TensorLy，选择它的原因主要因为它文档写得详细… QuickSort1.What is TensorLy传送门在这里。TensorLy是一个简单快速的Python张量库，优点在于纯Python实现，依赖少，并且能够支持如Numpy、MXnet多种数据的底层。最重要的是文档详细。 2.Basic Operations1.Import the library首先要引入相应的库才能使用 12import numpy as npimport tensorly as tl 2.Create a tensor使用numpy生成一个三维数组，并构建成TensorLy中的张量 1tensor = tl.tensor(np.arange(24).reshape((3, 4, 2))) 3.Unfolding tensor by mode将张量按维度展开 12import tensorly as unfoldunfold(tensor, 0) # mode-1 unfolding 4.Folding tensor将展开的数组按维度还原成张量 1234from tensorly import foldunfolding = unfold(tensor, 1)original_shape = tensor.shapefold(unfolding, 1, original_shape) 3.Tensor decomposition张量分解有很多种分解方法，但主要的分解方法有两种：Tucker分解和CP分解，其中CP分解是Tucker分解的一个特例。TensorLy也提供了这两种方法的实现。Tucker分解的公式为 X = S * A * B * C，X是原张量，S为分解之后产生的核心张量，A、B、C分别为分解之后关于3个维度的矩阵。图示如下： 1234from tensorly.decomposition import tucker# tucker函数传入一个张量，并指定核心张量的各个维度的长度# tucker函数的返回结果是分解完成之后的核心张量，和各个维度的矩阵core, factors = tucker(tensor, ranks=[2, 3, 1]) CP分解是Tucker分解的特例。如果Tucker分解中的核心张量是一个超对角结构（对角，并且三个维度的长度相等），则Tucker分解退化为CP分解。公式为 X = λ * A * B * C（带权重），X是原张量，λ矩阵为分解之后超对角张量的对角值，A、B、C分别为分解之后关于3个维度的矩阵。图示如下： 1234from tensorly.decomposition import parafac# parafac函数传入一个张量，并指定原张量的一个估计（近似）的秩# 返回的就是分解之后factors = parafac(tensor, rank=2) 4.Demo使用张量分解的的一个例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import matplotlib.pyplot as pltimport tensorly as tlimport numpy as npfrom tensorly.decomposition import parafacfrom tensorly.decomposition import tuckerfrom PIL import Imagerandom_state = 12345tl.set_backend(&#x27;numpy&#x27;)# 打开图片image_png = Image.open(&#x27;/Users/orechou/Downloads/index_1.jpg&#x27;)# 打印图片的信息print(image_png)data = np.reshape(image_png, [1000, 1500, 3])# 将值从int转变成float，因为在分解的过程中会产生浮点数image = tl.tensor(data, dtype=&#x27;float64&#x27;)def to_image(tensor): &quot;&quot;&quot;A convenience function to convert from a float dtype back to uint8&quot;&quot;&quot; im = tl.to_numpy(tensor) im -= im.min() im /= im.max() im *= 255 return im.astype(np.uint8)# Rank of the CP decompositioncp_rank = 25# Rank of the Tucker decompositiontucker_rank = [100, 100, 2]# Perform the CP decompositionfactors = parafac(image, rank=cp_rank, init=&#x27;random&#x27;, tol=10e-6)# Reconstruct the image from the factorscp_reconstruction = tl.kruskal_to_tensor(factors)# Tucker decompositioncore, tucker_factors = tucker(image, ranks=tucker_rank, init=&#x27;random&#x27;, tol=10e-5, random_state=random_state)tucker_reconstruction = tl.tucker_to_tensor(core, tucker_factors)# Plotting the original and reconstruction from the decompositionsfig = plt.figure()ax = fig.add_subplot(1, 3, 1)ax.set_axis_off()ax.imshow(to_image(image))ax.set_title(&#x27;Original&#x27;)ax = fig.add_subplot(1, 3, 2)ax.set_axis_off()ax.imshow(to_image(cp_reconstruction))ax.set_title(&#x27;CP&#x27;)ax = fig.add_subplot(1, 3, 3)ax.set_axis_off()ax.imshow(to_image(tucker_reconstruction))ax.set_title(&#x27;Tucker&#x27;)plt.tight_layout()plt.show() 效果如下："},{"title":"Spring Web Flow","date":"2022-08-20T05:46:15.407Z","updated":"2016-11-21T03:34:27.000Z","comments":true,"path":"back_posts/old_posts/spring_flow.html","permalink":"https://orechou.github.io/back_posts/old_posts/spring_flow.html","excerpt":"","text":"本Blog内容主要基于《Spring In Action》第四版和Github上面的sample。The first production ready 1.0 release was made on 2006-10-26. Version 2.0, first released on 2008-04-29。从那之后就没有大的版本更新。 1、在Spring中配置Web Flow《Spring In Action》第四版说“还不支持在Java中配置Spring Web Flow”，但是实际上已经支持了Java的配置。主要在代码中继承并实现AbstractFlowConfiguration接口。首先做好SpringMVC的Java配置。因为Spring Web Flow是构建于SpringMVC基础之上，所有的请求都需要首先经过SpringMVC的DispatcherServlet。（1）装配流程执行器实现一个继承了AbstractFlowConfiguration的类。 @Bean public FlowExecutor flowExecutor() &#123; return getFlowExecutorBuilder(flowRegistry()) .addFlowExecutionListener(new SecurityFlowExecutionListener(), &quot;*&quot;) .build(); &#125;流程执行器驱动流程的执行，当用户进入一个流程时，流程执行器会为用户创建并启动一个流程执行实例。（2）配置流程注册表加载流程定义并让流程执行器能够使用它们。 @Bean public FlowDefinitionRegistry flowRegistry() &#123; return getFlowDefinitionRegistryBuilder(flowBuilderServices()) .setBasePath(&quot;/WEB-INF&quot;) .addFlowLocationPattern(&quot;/**/*-flow.xml&quot;).build(); &#125;（3）处理流程请求需要配置一个FlowHandlerMapping来帮助DispatcherServlet将流程的请求发送给Spring Web Flow。 @Bean public FlowHandlerMapping flowHandlerMapping() &#123; FlowHandlerMapping handlerMapping = new FlowHandlerMapping(); handlerMapping.setOrder(-1); handlerMapping.setFlowRegistry(this.webFlowConfig.flowRegistry()); return handlerMapping; &#125;这里面装配了流程注册表，这样就能够知道如何将请求的URL匹配到流程上。配置这个Bean的作用只是将流程请求定向到swf，但是并不做响应。 @Bean public FlowHandlerAdapter flowHandlerAdapter() &#123; FlowHandlerAdapter handlerAdapter = new FlowHandlerAdapter(); handlerAdapter.setFlowExecutor(this.webFlowConfig.flowExecutor()); handlerAdapter.setSaveOutputToFlashScopeOnRedirect(true); return handlerAdapter; &#125;我们可以看到这里面设置了流程的执行器，类似于SpringMVC中的控制器，它会响应发送的请求，并对其进行处理。 2、流程的组件在Spring Web Flow中，流程是由状态(state)、转移(transition)、流程数据(data)三个元素定义。状态类型：行为(action)、决策（decision）、结束（end）、子流程(subflow)、视图(view)(视图状态用于为用户展现信息并使用户在流程中发挥作用，也即是用于交互)视图状态id:标示这个状态；若在这个状态中没有用view指定视图，则这个id也代表了视图名。model:将会把id视图中的表单数据绑定到model中所指定的对象。行为状态会出发spring中所管理bean的一些方法，方法执行调用之后会转移到另一个状态。决策状态若流程不是线性的，则需要使用决策状态子流程状态结束状态用于流程的结束。当结束的是子流程，子流程结束之后，调用它的流程将会从调用处继续执行。 状态之间的迁移to用于指定流程的下一个状态on用于指定触发转移的事件on-exception用于制定了要转移的异常&lt;global-trasitions&gt;全局转移，对于定义的流程转移，所有的状态都会默认拥有这个转移 3、流程数据可以在流程中定义变量，定义的变量可以在流程中的各个地方进行引用。&lt;var name=&quot;&quot; class=&quot;&quot;&gt;"},{"title":"服务器性能指标","date":"2022-08-20T05:46:15.407Z","updated":"2018-09-15T07:19:30.000Z","comments":true,"path":"back_posts/old_posts/服务器性能指标.html","permalink":"https://orechou.github.io/back_posts/old_posts/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87.html","excerpt":"在对互联网服务进行服务端性能测试时，主要关注两方面的性能指标： 业务指标：如吞吐量(QPS、TPS)、响应时间(RT)、并发数、业务成功率等 资源指标：如 CPU、内存、Disk I/O、Network I/O等资源的消耗情况本文主要介绍一些广泛适用的、基本的资源指标以及这些指标在 Linux 服务器的获取方式。","text":"在对互联网服务进行服务端性能测试时，主要关注两方面的性能指标： 业务指标：如吞吐量(QPS、TPS)、响应时间(RT)、并发数、业务成功率等 资源指标：如 CPU、内存、Disk I/O、Network I/O等资源的消耗情况本文主要介绍一些广泛适用的、基本的资源指标以及这些指标在 Linux 服务器的获取方式。 服务器性能指标CPU 相关负载 （Load）在类 Unix 系统中，系统负载是对当前 CPU 工作量的度量，定义为特定时间间隔内运行队列中的平均线程数。 常用命令Linux 上查看机器负载的指令如下： uptime 命令：打印系统总共运行了多长时间和系统的平均负载（1 分钟、5 分钟、15 分钟） w 命令：显示目前登入系统的用户信息，系统最近 1 分钟、5 分钟、15 分钟的平均负载 top 命令：实时显示系统中各个进程的资源占用状态 通过将系统 1 分钟、5 分钟、15 分钟的平均负载称为 load1、load5、load15 Java 应用导致 load 飚升的排查负载过高可能的原因： 内存泄露导致频繁的 GC 出现了死锁的情况 有大字段的读写，IO 若线上 load ，可考虑将堆栈内存 dump ，重启系统，零时解决。解决的步骤如下： 查看 load 情况首先使用 uptime 查看当前系统的 load 12 OreChou@orechoudeMacBook-Pro $ uptime17:35 up 10 days, 20:09, 3 users, load averages: 3.05 3.92 3.35 查看占用 CPU 较高进程使用 top 命令，查看 CPU 占用较高的进程 ID 查看占用 CPU 较高线程因为 linux 的内核中，进程与线程的数据结构相同。所以通过进程 ID 查找当前进程中占用 CPU 较高的线程 ID。使用指令 top -Hp pid。 查看该线程的栈空间因为 JVM 线程对应的就是一个 linux 内核中的线程，所以可以通过 jstack 查看该线程的方法栈，找到正在执行的方法。使用命令 jstack pid | grep -A 200 tid 利用率（Usage）在类 Unix 系统中，利用率是对当前 CPU 工作量的度量，定义为特定时间间隔内 CPU 被占用的情况。 指标 说明 备注 us 用户进程执行时间百分比 us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy 内核系统进程执行时间百分比 sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 id 空闲时间百分比 wa IO 等待时间百分比 wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 st 虚拟 CPU 等待实际 CPU 的时间的百分比 常用命令Linux 上查看机器 CPU 利用率的命令如下： vmstat 命令：展现给定时间间隔的服务器的状态值。包括服务器的CPU 使用率，内存使用，虚拟内存交换情况,IO 读写情况。 top 命令：实时显示系统中各个进程的资源占用状态。 12345$ vmstat 时间间隔 采样次数procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 2479444 0 3165172 0 0 196 15905 2 8 0 0 84 5 11 0 0 0 2479404 0 3165176 0 0 0 2804 81664 2715 0 0 90 1 9 Java 应用导致 Usage 飚升的排查Java 应用导致 CPU 利用率过高的原因： 内存泄露导致大量的 GC 代码存在死循环 排查的方法同上。 内存相关Linux 中的内存可以分成三个部分，物理内存、虚拟内存和 Swap 分区（交换区）。 物理内存：内存条那部分内存空间，实际的内存空间。 虚拟内存：是计算机系统内存管理的一种技术。使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存的使用也更有效率。 Swap 分区：当系统物理内存的不够用的时候，把物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到 Swap 分区中（磁盘上），等到那些程序要运行时，再从Swap 分区中恢复保存的数据到内存中。 内存相关的性能指标主要是，内存的使用率。 使用率（Usage）在上面的概念中物理内存和 Swap 分区是实际的物理的空间。描述内存使用的属性如下： 指标 描述 备注 total 物理内存总大小 used 已经分配的内存的大小（包括了 shared、buffers 和 cached） free 未被分配的内存的大小 shared 共享内存 buffers 存放要输出到 disk（块设备）的数据，缓冲满了一次写，提高IO性能（内存 -&gt; 磁盘） cached 存放从 disk 上读出的数据，常用的缓存起来，减少IO（磁盘 -&gt; 内存） 常用命令Linux 上查看内存使用的命令如下： free 命令：显示 Linux 系统中空闲的、已用的物理内存及 swap 内存，及被内核使用的 buffer 等 top 命令：top 命令是很全能的，实时显示系统中各个进程的资源占用状态。 12345$free -m total used free shared buffers cachedMem: 8192 2802 5389 0 0 1559-/+ buffers/cache: 1243 6948Swap: 16383 0 16383 可以在 linux 下使用 /proc/meminfo 文件查看操作系统的使用状态。free 也是基于 /proc/meminfo 文件的内容实现的。 Java 内存泄露排查思路获取内存占用较高的 PID使用 jmap 命令1234567jmap -heap 3331：查看java 堆（heap）使用情况jmap -histo 3331：查看堆内存(histogram)中的对象数量及大小jmap -histo:live 3331：JVM会先触发gc，然后再统计信息jmap -dump:format=b,file=heapDump 3331：将内存使用的详细情况输出到文件 也可以得到 dump 文件，再进行分析。 页面交换（Paging）页面交换包括从 Swap 交换到内存和从内存交换到 Swap，如果系统出现频繁的页面交换，需要引起注意。可以从 vmstat 中的 si 和 so 指标可以查看： si：每秒从 Swap 读取到内存的数据大小 so：每秒从内存写入到 Swap 的数据大小"},{"title":"OSWorkflow","date":"2022-08-20T05:46:16.860Z","updated":"2016-11-20T09:38:34.000Z","comments":true,"path":"back_posts/old_posts/解析tern工作流.html","permalink":"https://orechou.github.io/back_posts/old_posts/%E8%A7%A3%E6%9E%90tern%E5%B7%A5%E4%BD%9C%E6%B5%81.html","excerpt":"","text":"特点：没有好的可视化工具，2.8.0是最新的版本，但这已经是07年的版本。目前官网已经没有对他提供更新。原理：主要基于有限状态机（Finite state machine），将每一个状态描述成流程中的一个步骤。jar包：osworkflow、oscore、propertySet。 1、基本概念工作流描述文件，一个xml文件。wrokflow:一个根元素，代表了一个工作流。一般由多个step构成，step表示一个流程。step:每一个步骤中可以有一个或者多个action，action表示一个动作。每执行完一个步骤，就会走向下一个步骤。在不同的步骤之中流程的状态可以不断的改变，状态类别可以设置。action:每一个动作可以有一个无条件结果unconditional result和零到多个conditional result条件结果。在result中指定完成之后的状态，并指定跳转的下一个步骤。在beanshell 表达式范围内中始终存在三个变量，entry、context和store，分别是WorkflowEntry、WorkflowContext、WorkflowStore三个对象。beanshell是一种完全符合java语法的脚本语言，可以理解为可以在beanshell中编写java代码。 2、实现细节1、配置数据库，这是你工作流中数据流转的数据库来源。可以选择xml配置模式。在传统的动态web工程的server.xml文件中，设置好数据库驱动。Tern将所有配置信息保存在Java代码中。2、持久化（Persistence）。内置提供了：MemoryStore、SerializableStore、JDBCStore方式。也可以自己继承WorkflowStore接口，实现自己的需求。3、载入流程定义文件（WorkflowFactory）读取编写好的xml，将流程载入。可以继承AbstractWorkflowFactory来实现自己的需求。 3、Tern框架里实现1、流程服务（Service.class）Service用于定义好流程的ID，流程的名称 ，流程所关联的实体数据表名称。此处的流程不是指的具体的流程，而是指的一种流程。主要的属性：流程ID，流程名称，流程描述，流程关联的实体数据（可以有实体类类名，也可以是该实体类在数据库中的表名）。在后面新建立一个流程的时候都要通过这个类去获取一种流程，然后新增一个流程实例。2、流程实例（Process.class）Process用于定义一个具体的流程。也即是是通过Service生成的一个流程实例。主要的属性：流程ID，流程服务ID，实体数据ID，流程描述，发起者ID，发起时间，结束时间，序列号，状态3、工作流记录（WorkflowEntry.class）WorkflowEntry用于定义描述一个工作流实例。主要属性：工作流名称、工作流状态、是否已经初始化、id在tern中对WorkflowEntry进行了扩展，增加了record(实体数据记录)、user(当前操作员)、service(该工作流对应的流程服务)、process(该工作流的流程实例)WorkflowEntry的id在新建的时候被赋值为process的id4、工作流持久化（WorkflowStore）用于数据的持久化。tern扩展继承了JDBCWorkflowStore。主要的功能是负责管理流程的执行过程。主要的功能函数：findSteps去获取数据库中已经进行的流程步骤。可传入多种参数entryId，stategetNextEntrySequence获取下一步流程的序列号createEntry新增一个WorkflowEntry实例findCurrentSteps();findHistorySteps();createCurrentStep();tern在数据库中建立了表wf_stepinfo，上面的方法都是对wf_stepinfo进行操作。5、工作流工厂主要的负责流程的定义方面。WorkflowDescriptor在OSWorkflow中表示一个处理之后的流程6、工作流引擎（Workflow.class）tern实现了OSWorkflow提供的工作流引擎的接口。tern将工作流引擎设计为单例模式。引擎中设置了环境上下文(Context)，里面存储了当前登录的操作员。引擎中持有了一个运行当前工作流的线程对象。主要的功能函数：（1）createInstance()：创建工作流的实例，即通过Service和实体数据record创建一个流程。这一步会在数据库中getAvailableActions()：函数中根据WorkflowStore，IAPWorkflowEntry，WorkflowDescriptor这三个对象，获取到当前步骤中可做的动作ActionsgetPermissionHandler()：根据传入的WorkflowDescriptor和工作流引擎中存储的操作员，可以判断出当前操作员的角色是否有权限参与流程getCurrentSteps();getEntryState();getHistorySteps();：调用了WorkflowStore里面的方法initialize()：新增一个工作流条目，changeEntryState()：变更工作流条目的状态COMPLETED,CREATED,ACTIVATED,SUSPENDED,KILLED。doAction()：做工作流里面的操作。entry、propertySet、transientVars这些变量可通过"},{"title":"JavaScript MVVM 双向绑定的实现","date":"2022-08-20T05:46:17.415Z","updated":"2018-06-20T14:02:15.000Z","comments":true,"path":"back_posts/old_posts/MVVM双向绑定的实现.html","permalink":"https://orechou.github.io/back_posts/old_posts/MVVM%E5%8F%8C%E5%90%91%E7%BB%91%E5%AE%9A%E7%9A%84%E5%AE%9E%E7%8E%B0.html","excerpt":"这篇文章主要记录学习 JS 双向绑定过程中的一些概念与具体的实现 MVVM 具体概念MVVM 中有一些概念是通用的，具体如下 Directive （指令）自定义的执行函数，例如 Vue 中的 v-click、v-bind 等。这些函数封装了 DOM 的一些基本可复用函数API。","text":"这篇文章主要记录学习 JS 双向绑定过程中的一些概念与具体的实现 MVVM 具体概念MVVM 中有一些概念是通用的，具体如下 Directive （指令）自定义的执行函数，例如 Vue 中的 v-click、v-bind 等。这些函数封装了 DOM 的一些基本可复用函数API。 Filter （过滤器）用户希望对传入的初始数据进行处理，然后将处理结果交给 Directive 或者下一个 Filter。例如：v-bind=”time | formatTime”。formatTime 是将 time 转换成指定格式的 Filter 函数。 表达式类似前端普通的页面模板表达式，作用是控制页面内容安装具体的条件显示。例如：if…else 等 ViewModel传入的 Model 数据在内存中存放，提供一些基本的操作 API 给开发者，使其能够对数据进行读取与修改 双向绑定（数据变更检测）View 层的变化改变 Model：通过给元素添加 onchange 事件来触发对 Model 数据进行修改 Model 层的变化改变 View： 手动触发绑定 脏数据检测 对象劫持 Proxy 实现方式手动触发绑定即 Model 对象改变之后，需要显示的去触发 View 的更新 首先编写 HTML 页面 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Two way binding&lt;/title&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 这里定义了两个元素，input 元素用于接受用户输入，span 元素用于显示 --&gt; &lt;input q-value=&quot;value&quot; type=&quot;text&quot; id=&quot;input&quot;&gt; &lt;span q-text=&quot;value&quot; id=&quot;el&quot;&gt;&lt;/span&gt;&lt;/body&gt;&lt;script src=&quot;way_1.js&quot;&gt;&lt;/script&gt;&lt;/html&gt; 编写实现 MVVM 的 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Manual triggerlet elems = [document.getElementById(&#x27;el&#x27;), document.getElementById(&#x27;input&#x27;)]// 数据 Modellet data = &#123; value: &#x27;hello&#x27;&#125;// 定义 Directivelet directive = &#123; text: function(text) &#123; this.innerHTML = text &#125;, value: function(value) &#123; this.setAttribute(&#x27;value&#x27;, value) this.value = value &#125;&#125;// 扫描所有的元素function scan() &#123; // 扫描带指令的节点属性 for (let elem of elems) &#123; elem.directive = [] for (let attr of elem.attributes) &#123; if (attr.nodeName.indexOf(&#x27;q-&#x27;) &gt;= 0) &#123; directive[attr.nodeName.slice(2)].call(elem, data[attr.nodeValue]) elem.directive.push(attr.nodeName.slice(2)) &#125; &#125; &#125;&#125;// ViewModel 更新函数function ViewModelSet(key, value) &#123; // 修改数据对象后 data[key] = value // 手动地去触发 View 的修改 scan()&#125;// View 绑定监听elems[1].addEventListener(&#x27;keyup&#x27;, function(e) &#123; ViewModelSet(&#x27;value&#x27;, e.target.value)&#125;, false)// -------- 程序执行 -------scan()setTimeout(() =&gt; &#123; ViewModelSet(&#x27;value&#x27;, &#x27;hello world&#x27;)&#125;, 1000); 数据劫持数据劫持是目前比较广泛的方式，Vue 的双向绑定就是通过数据劫持实现。实现方式是通过 Object.defineProperty 和 Object.defineProperies 方法对 Model 对象的 get 和 set 函数进行监听。当有数据读取或赋值操作时，扫描（或者通知）对应的元素执行 Directive 函数，实现 View 的刷新。 HTML 的代码不变，js 代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// Hijackinglet elems = [document.getElementById(&#x27;el&#x27;), document.getElementById(&#x27;input&#x27;)]let data = &#123; value: &#x27;hello&#x27;&#125;// 定义 Directivelet directive = &#123; text: function(text) &#123; this.innerHTML = text &#125;, value: function(value) &#123; this.setAttribute(&#x27;value&#x27;, value) this.value = value &#125;&#125;// 定义对象属性设置劫持// obj: 指定的 Model 数据对象// propName: 指定的属性名称function defineGetAndSet(obj, propName) &#123; let bValue // 使用 Object.defineProperty 做数据劫持 Object.defineProperty(obj, propName, &#123; get: function() &#123; return bValue &#125;, set: function(value) &#123; bValue = value // 在 vue 中，这里不会去扫描所有的元素，而是通过订阅发布模式，通知那些订阅了该数据的 view 进行更新 scan() &#125;, enumerable: true, configurable: true &#125;)&#125;// View 绑定监听elems[1].addEventListener(&#x27;keyup&#x27;, function(e) &#123; data.value = e.target.value&#125;, false)// 扫描所有的元素function scan() &#123; // 扫描带指令的节点属性 for (let elem of elems) &#123; elem.directive = [] for (let attr of elem.attributes) &#123; if (attr.nodeName.indexOf(&#x27;q-&#x27;) &gt;= 0) &#123; directive[attr.nodeName.slice(2)].call(elem, data[attr.nodeValue]) elem.directive.push(attr.nodeName.slice(2)) &#125; &#125; &#125;&#125;// -------- 程序执行 -------scan()defineGetAndSet(data, &#x27;value&#x27;)setTimeout(() =&gt; &#123; // 这里为数据设置新值之后，在 set 方法中会去更新 view data.value = &#x27;Hello world&#x27;&#125;, 1000); 基于 Proxy 的实现Proxy 是 ES6 中的新特性。可以在已有的对象基础上定义一个新对象，并重新定义对象原型上的方法。例如 get 和 set 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Hijackinglet elems = [document.getElementById(&#x27;el&#x27;), document.getElementById(&#x27;input&#x27;)]// 定义 Directivelet directive = &#123; text: function(text) &#123; this.innerHTML = text &#125;, value: function(value) &#123; this.setAttribute(&#x27;value&#x27;, value) this.value = value &#125;&#125;// 设置对象的代理let data = new Proxy(&#123;&#125;, &#123; get: function(target, key, receiver) &#123; return target.value &#125;, set: function (target, key, value, receiver) &#123; target.value = value scan() return target.value &#125;&#125;)// View 绑定监听elems[1].addEventListener(&#x27;keyup&#x27;, function(e) &#123; data.value = e.target.value&#125;, false)// 扫描所有的元素function scan() &#123; // 扫描带指令的节点属性 for (let elem of elems) &#123; elem.directive = [] for (let attr of elem.attributes) &#123; if (attr.nodeName.indexOf(&#x27;q-&#x27;) &gt;= 0) &#123; directive[attr.nodeName.slice(2)].call(elem, data[attr.nodeValue]) elem.directive.push(attr.nodeName.slice(2)) &#125; &#125; &#125;&#125;// -------- 程序执行 -------data[&#x27;value&#x27;] = &#x27;Hello&#x27;scan()setTimeout(() =&gt; &#123; data.value = &#x27;Hello world&#x27;&#125;, 1000); 脏数据监测基本原理是在 Model 对象的属性值发生变化的时候找到与该属性值相关的所有元素，然后判断数据是否发生变化，若变化则更新 View。 编写页面代码如下： 123456789101112&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;Two way binding&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;input q-event=&quot;value&quot; q-bind=&quot;value&quot; type=&quot;text&quot; id=&quot;input&quot;&gt; &lt;span q-event=&quot;text&quot; q-bind=&quot;value&quot; id=&quot;el&quot;&gt;&lt;/span&gt;&lt;/body&gt;&lt;script src=&quot;way_2.js&quot;&gt;&lt;/script&gt;&lt;/html&gt; js 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// Dirty detectionlet elems = [document.getElementById(&#x27;el&#x27;), document.getElementById(&#x27;input&#x27;)]let data = &#123; value: &#x27;hello&#x27;&#125;// 定义 Directivelet directive = &#123; text: function(text) &#123; this.innerHTML = text &#125;, value: function(value) &#123; this.setAttribute(&#x27;value&#x27;, value) this.value = value &#125;&#125;// 脏数据循环检测function digest(elems) &#123; for (let elem of elems) &#123; if (elem.directive === undefined) &#123; elem.directive = &#123;&#125; &#125; for (let attr of elem.attributes) &#123; if (attr.nodeName.indexOf(&#x27;q-event&#x27;) &gt;= 0) &#123; let dataKey = elem.getAttribute(&#x27;q-bind&#x27;) || undefined // 进行脏数据检测，如果数据改变，则重新执行命令 if (elem.directive[attr.nodeValue] !== data[dataKey]) &#123; directive[attr.nodeValue].call(elem, data[dataKey]) elem.directive[attr.nodeValue] = data[dataKey] &#125; &#125; &#125; &#125;&#125;// 数据监听function $digest(value) &#123; let list = document.querySelectorAll(&#x27;[q-bind=&#x27; + value + &#x27;]&#x27;) digest(list)&#125;// View 绑定监听elems[1].addEventListener(&#x27;keyup&#x27;, function(e) &#123; data.value = e.target.value $digest(e.target.getAttribute(&#x27;q-bind&#x27;))&#125;, false)// -------- 程序执行 -------$digest(&#x27;value&#x27;)setTimeout(() =&gt; &#123; data.value = &quot;Hello world&quot; $digest(&#x27;value&#x27;)&#125;, 1000); 总结上面只是简单地实现了双向绑定，但实际上一个完整的 MVVM 框架要考虑很多东西。在上面的实现中数据劫持的方法更新View 是使用了 Scan 函数，但实际的实现中（比如 Vue）是使用了发布订阅的模式。它只会去更新那些与该 Model 数据绑定的元素，而不会去扫描所有元素。而在脏数据检测中，它去找到了所有绑定的元素，然后判断数据是否发生变化，这种方式只有一定的性能开销的。 参考《现代前端技术解析》 代码下载"},{"title":"Sept-11-2016","date":"2022-08-20T05:46:17.416Z","updated":"2016-09-11T15:29:54.000Z","comments":true,"path":"back_posts/old_posts/Sept-11-2016.html","permalink":"https://orechou.github.io/back_posts/old_posts/Sept-11-2016.html","excerpt":"","text":"暑假回家了10天，在学校体验了几周的早班地铁，体会了挤地铁的艰辛。本来有说的技术博客也变成了一两个月的牢骚。明天开学第一课，是时候再发一波骚气了。 关于项目：CPMS系统开发一个MIS系统，都要针对该MIS系统所设计的行业有一个比较透彻的了解，要补习一些IT行业外的知识。另外开发任何一个系统，都要与业主有很好的沟通。参与这个项目，我的感觉是沟通不够。两个问题，我没有主动沟通的积极性，二是没有对沟通的有一定的重视程度。沟通和另外的一些原因于，导致该项目的停滞不前、进度缓慢。 关于研究生9月来学校，我去了一趟虎溪——大学开始的地方。没有多逛，从大学城地铁站、走熙街、穿过北门南门、最后回沙坪坝。回忆起本科这四年，业精于勤荒于嬉。对于后面这三年，我想，是该骚气地做一波改变了。 关于写东西每次想写，都写不了多少。。。"},{"title":"Tern Model层解析","date":"2022-08-20T05:46:16.931Z","updated":"2016-11-08T08:54:26.000Z","comments":true,"path":"back_posts/old_posts/解析tern框架Model层.html","permalink":"https://orechou.github.io/back_posts/old_posts/%E8%A7%A3%E6%9E%90tern%E6%A1%86%E6%9E%B6Model%E5%B1%82.html","excerpt":"","text":"com.tern.dbdb对JDBC做了封装，提供了一系列的对数据库的操作。函数提供了链式的操作。db获取app.yml中的配置进行数据库的连接(ConnectionPool类)，支持了如MySql、Oracle、Sqlite等多种数据库。查询等获取的数据库的数据以DataTable（返回一张表的数据），或者以DataRow（返回一条数据行的数据）\u0010作为对象返回。 com.tern.daodao是对db的再一次做的封装，从而满足MVC中的Model层的要求。\u0010 1、Modeldao中的核心类之一，实现了数据模型的封装。其定义了数据表名称、所包含的列（属性）的信息（如：列名、列数据类型、约束条件）和与其他模型之间的关系信息。MoodelReader是模型的读取器（抽象类），DefaultModelReader（数据库元数据读取）、YamlModelReader（Yaml文件读取）、DBModelReader（数据库读取）是三个实现。 获取Model的方法form(String name, Database db)这是一个静态工厂方法，它实际去调用protected声明的构造方法Model(String name, Database h)，该构造方法中，首先尝试去从yaml中读取数据模型的schema，若不存在的话才会去数据库中获取schema。构造方法中调用ModelReader的实现类去读取schema，读取columns的信息、caption、title等数据并解析（这些数据都是以键值对的形式作存储，map，方便解析)。在元数据库中存储了对于Model定义、Model的列/字段定义、Model之间的关系定义、关系中的字段映射关系的数据，分别为iap_entities、iap_columns、iap_relations、iap_relation_map这几张表。 2、Record抽象意义是一条数据记录。Recrod是一个实现了Map&lt;String, Object&gt;接口的类。每一个Record类的内部都持有该数据记录的model，row即是该Record数据中的所有列的键值对。有这两个内部实例，Record类即可表示所有定义了Model的实际数据库对象。 3、RecordSet抽象意义是数据记录的集合。其持有数据模型model，还有一个Query类（db提供的数据库查询的工具类）。通过model中的表名称，和Query类，RecordSet可以查询数据库中并返回记录。返回的记录以RecordSet的对象存储。RecordSet定义了很多访问数据库返回数据的方法。 4、Model中的CURDModel中提供了query(String where, Object[] params)等方法，读取数据库，返回RecordSet数据。其实是调用了RecordSet的方法。另外Model提供了新增、删除、修改的方法。 总结com.tern.db的封装是提供一种访问数据库的简便方法，com.tern.dao的封装是提供一种操作模型简便方法。简单来说，在实际开发之中设计好数据库之后，可以先行一步配置好Model，tern平台提供了ide的网页平台将Model录入进元数据库中。这样做的好处是，不用在实际的项目之中再编写entity类，因为模型model已经存在，而且得益于record的抽象。Model可以去数据库读取数据，而返回的数据可以用record的容纳。"},{"title":"数据库相关总结","date":"2022-08-20T05:46:17.262Z","updated":"2018-05-03T13:19:02.000Z","comments":true,"path":"back_posts/old_posts/数据库相关总结.html","permalink":"https://orechou.github.io/back_posts/old_posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93.html","excerpt":"","text":"事务ACID四个单词的首字母组合：Atomicity、Consistency、Isolation、Durability Atomicity（原子性）：一个事务必须被视为一个最小不可分割的工作单元，事务中的操作要么全部执行提交完成成功，要么全部失败回滚。 Consistency（一致性）：系统的状态总是从一个一致性的状态转换到另外一个一致性状态。 Isolation（隔离性）：一个事务中所有的操作在成功提交之前，对其他的事务是不可见的。 Durability（持久性）： 数据库引擎文件系统中，Mysql将每个数据库（Schema）保存为数据库目录下的一个子目录。创建表时，Mysql在数据库子目录下创建一个和表同名的.frm文件保存表的定义。 例如：主键的Auto increment，是在.frm有个Auto_increment属性记录了下一个AUTO_INCREMENT的值。 Schema与基础类型优化VARCHAR CHARVARCHAR存储可变长字符串（VARCHAR需要使用1或2个额外字节记录字符串长度。列长度小于等于255，使用1个字节，否则两个字节），CHAR存储固定长度字符串。 VARCHAR由于是变长，能够减少存储的空间。但Update时，数据行可能会变长导致做额外的工作。例如：行占用空间增大，页内无法存储。InnoDB会采取分页来使行可以放进页内。InnoDB会将过长的Varchar存储为BLOB 适合VARCHAR的情况：字符串列的最大长度比平均长度大很多；列的更新少，碎片不成问题；使用了像UTF-8这样复杂的字符集，每个字符都使用了不同的字节数存储。 适合CHAR的情况：存储很短的字符串；所有的字符串接近同一个长度。例如：适合存储密码的MD5的值；存储经常变更的值。 BLOB 和 TEXT两者都是为存储很大的数据而设计的字符串数据类型，分别采用二进制和字符串方式存储。所以TEXT有字符集和排序规则，而BLOB没有。Mysql把每个BLOB和TEXT值当做一个独立的对象处理。当BLOB和TEXT值太大时，InnoDB会使用外部的存储区域来存储实际的值，此时在行中存储一个1~4字节的指针。BLOB和TEXT两种属性的列不能进行索引。 DATETIME &amp; TIMESTAMPDATETIME能保存大范围的值，从1001年到9999年，精度为秒。8个字节。TIMESTAMP保存小范围的值，从1970年到2038年，精度为秒。4个字节。同UNIX时间戳。 应尽量使用TIMESTAMP，其比DATETIME空间效率更高。 索引查询类型全值匹配：和索引中所有列进行匹配匹配最左前缀：只使用索引的第一列匹配列前缀：只匹配某一列的值的开头部分匹配范围值：某一类的值按一个范围匹配 优缺点优点 减少了服务器扫描的数据量 帮助服务器避免排序和零时表 将随机IO变成顺序IO缺点 增加了磁盘空间的使用 增加了数据增删查改的操作时间（要维护索引）"},{"title":"Java NIO","date":"2022-08-20T05:46:17.416Z","updated":"2018-09-16T14:52:38.000Z","comments":true,"path":"back_posts/old_posts/Java/Java NIO.html","permalink":"https://orechou.github.io/back_posts/old_posts/Java/Java%20NIO.html","excerpt":"Java IO 相关的文章，在网络上可以搜索到一大堆，并且 《Java NIO》和《Netty 权威指南》两本书可以包读者从入门到进阶。写这篇文章的目的在于让自己总结一下 Java IO 知识，从总结中学习。 Java 的 IO 大致可以分为如下几类： 磁盘操作：File 字节操作：InputStream &amp; OutputStream 字符操作：Reader &amp; Writer 网络操作：Socket 最开始 Java 只提供了 BIO（Block IO），Block IO 的意思是线程调用 IO 操作的时候，线程会阻塞等待 IO 完成。在这个期间，线程除了等待无法完成其他的事情。因为这种 IO 模式，之前的 Java 框架在编写服务器的时候，会对每一个 Socket 的请求新建一个线程进行处理。因为线程的新建与销毁是需要消耗系统资源的，所以若系统频繁地创建与销毁线程，那么会对系统资源造成很大的浪费。在这种 IO 模式下的改进方式是利用池化技术，通过线程池的方式来管理线程，减少线程频繁创建和销毁的开销。但是并没有改变最根本的问题，即一个服务请求的连接都需要一个线程来处理，在高并发的情况下，阻塞 IO 的实现肯定无法满足性能的需求。 针对这一问题，在 JDK 1.4 中 NIO 就登场了。","text":"Java IO 相关的文章，在网络上可以搜索到一大堆，并且 《Java NIO》和《Netty 权威指南》两本书可以包读者从入门到进阶。写这篇文章的目的在于让自己总结一下 Java IO 知识，从总结中学习。 Java 的 IO 大致可以分为如下几类： 磁盘操作：File 字节操作：InputStream &amp; OutputStream 字符操作：Reader &amp; Writer 网络操作：Socket 最开始 Java 只提供了 BIO（Block IO），Block IO 的意思是线程调用 IO 操作的时候，线程会阻塞等待 IO 完成。在这个期间，线程除了等待无法完成其他的事情。因为这种 IO 模式，之前的 Java 框架在编写服务器的时候，会对每一个 Socket 的请求新建一个线程进行处理。因为线程的新建与销毁是需要消耗系统资源的，所以若系统频繁地创建与销毁线程，那么会对系统资源造成很大的浪费。在这种 IO 模式下的改进方式是利用池化技术，通过线程池的方式来管理线程，减少线程频繁创建和销毁的开销。但是并没有改变最根本的问题，即一个服务请求的连接都需要一个线程来处理，在高并发的情况下，阻塞 IO 的实现肯定无法满足性能的需求。 针对这一问题，在 JDK 1.4 中 NIO 就登场了。 很多人将 NIO 称为 New IO 或者 Not Block IO，后者应该更贴切一点。 首先是 NIO 里面有哪些东西。 基本组件Buffer（缓冲区）NIO 处理的数据存储在 Buffer 里，一个 Buffer 对象是一个固定数量的数据容器，也可以视作数据传输的来源或者目标。 基本属性缓冲区的几个属性如下： Capacity（容量）：在创建时设定，无法在后续修改。表示能够容纳的数据元素的最大数量。 Limit（上界）：缓冲区中现存元素的计数，也表示了第一个不能读写的元素的位置。 Position（位置）：下一个要读或者要写的位置。 Mark（标记）：一个备忘的位置。 过程新建 Buffer假设创建一个容量为 10 的缓冲区。 12345ByteBuffer buffer = ByteBuffer.allocateDirect(10);// capacity: 10// limit: 10// position: 0// mark: x 填充 Buffer向缓冲区填充 5 次数据。 12345buffer.put((byte)&#x27;H&#x27;).put((byte)&#x27;e&#x27;).put((byte)&#x27;l&#x27;).put((byte)&#x27;l&#x27;).put((byte)&#x27;o&#x27;);// capacity: 10// limit: 10// position: 5 (即可视为当前 put 到数据中的数量，也可视为下个元素 put 的位置)// mark: x 翻转 Buffer当我们要去读取 Buffer 的时候，我们需要翻转（flip） Buffer。 123buffer.flip();// 代码等价于如下，即把 limit 位置设置成 position的位置，再将 position 设置为 0buffer.limit(buffer.position()).position(0); 压缩 Buffer当读取缓冲区中两个元素之后，缓冲区如下。 此刻从缓冲区中释放一部分数据，而不是全部，然后重新填充。为了实现这一点，未读的数据元素需要下移以使第一个元素索引为 0。从而将 Buffer 进行压缩（compact）。 1buffer.compact(); 根据图示，数据元素 2-5 被复制到 0-3 位置，4 以后的位置超出了 position，所以在后面 buffer 写入数据的时候被覆盖掉。 标记 Buffer使用标记函数将 mark 设置为当前 position 的值。当我们需要重复读取 Buffer 中某段数据的时候会可以派上用场。 1buffer.position(2).mark().position(4); 通道（Channel）通道位于缓冲区与通信的另一方（文件或 socket）之间，提供全双工的数据传输。JDK 1.4 提供的通道种类如下： FileChannel SocketChannel ServerSocketChannel DatagramChannel 打开 FileChannel 必须通过在 RandomAccessFile、FileInputStream 和 FileOutputStream 对象上调用 getChannel。而另外三个关于 socket 的通道则可以通过相应的静态工厂方法打开。原因在于，创建文件通道的时候，一定要有明确的文件目标，即先有了目标文件才去与之建立数据的通道。一个打开的文件通道对应了一个文件描述符。而开启 Socket 通道的时候还不知道谁来建立连接，与谁建立连接。 Socket 通道可以选择两种模式，阻塞模式和非阻塞模式。 12SocketChannel sc = SocketChannel.open( );sc.configureBlocking (false); // nonblocking NIO Socket 提供的非阻塞模式，是编写高性能 IO 应用的关键。对于 Java 编写的服务器来说，能够实现单线程对多个 Socket 连接的管理，并且方式简单。 选择器（Selector）通过将通道注册到选择器 Selector上，并绑定相关的事件，在后面就可以通过选择器找到这些通道就绪的相关事件。SelectionKey 封装了一个通道和选择器的注册关系。 123456789101112131415161718192021// step 1Selector selector = Selector.open();// step 2ServerSocketChannel ssChannel = ServerSocketChannel.open();ssChannel.configureBlocking(false); // 通道必须设置为非阻塞的模式ssChannel.register(selector, SelectionKey.OP_ACCEPT); // 这里注册了接受事件...// step 3// 当有就绪的通道的时候，我们可以调用 selectedKeys() 方法获取到这些 key，并进行处理int num = selector.select();Set&lt;SelectionKey&gt; keys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator();while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove();&#125; 事件的种类如下： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 1234567public static final int OP_READ = 1 &lt;&lt; 0;public static final int OP_WRITE = 1 &lt;&lt; 2;public static final int OP_CONNECT = 1 &lt;&lt; 3;public static final int OP_ACCEPT = 1 &lt;&lt; 4;// 可以看出每个事件可以被当成一个位域，从而组成事件集整数int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; Selector 的原理图如下： Reactor 模式当使用 NIO 编写 Java 服务器相关的程序的时候，通常会采用名为 Reactor 的设计模式。 Wikipedia: The reactor design pattern is an event handling pattern for handling service requests delivered concurrently by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to associated request handlers. 画个图如下所示： 从图中可以看出当有外部事件发生后，这些事件发送给 Service Handler，Service Handler 的作用是做一个分发，将这些请求分发给具体的 Handler 进行处理。 那么将 Reactor 模式应用到 NIO 的服务器编程上，可以有三种方案。 单线程 Reactor Reactor 线程要做很多事情，包括： 接受请求，注册事件 将就绪事件分别处理 这种模式下，Reactor 线程又要维护连接，又要做相应的工作。 带线程池的 Reactor 单线程 Reactor 模式，最直观的改进方式就是在事件处理的部分引入工作队列 + 线程池。我们将就绪任务进行分类，对每一种类别的任务，提交到专属的线程池中去处理。这样 Reactor 线程只需要做接受请求，注册监听事件的任务。而任务的处理提交给其他线程，不仅可以减小 Reactor 的压力，也能够提高任务处理的速度。 主从多 Reactor 使用一个 Reactor 线程来接受请求，注册监听所有的读写事件，当读写事件变多时，但线程可能无法满足性能的需求。可以引入了多 Reactor，也即一个主 Reactor 负责监控所有的连接请求，多个子 Reactor 负责监控并处理读/写请求，减轻了主 Reactor 的压力，降低了主 Reactor 压力太大而造成的延迟。并且每个子 Reactor 分别属于一个独立的线程，每个成功连接后的 Channel 的所有操作由同一个线程处理。这样保证了同一请求的所有状态和上下文在同一个线程中，避免了不必要的上下文切换，同时也方便了监控请求响应状态。 总结Java NIO 的实现基于底层操作系统提供的相关 API，例如 Linux 提供了 select、poll、epoll 等系统调用。Java NIO 无疑为编写的 IO 程序提高了性能，但是其缺点在于实现复杂，维护和管理困难。这也是 Netty 框架致力于解决的缺点。"},{"title":"Java 基础：详解 HashCode","date":"2022-08-20T05:46:17.498Z","updated":"2018-09-25T11:15:10.000Z","comments":true,"path":"back_posts/old_posts/Java/Java-基础详解-HashCode.html","permalink":"https://orechou.github.io/back_posts/old_posts/Java/Java-%E5%9F%BA%E7%A1%80%E8%AF%A6%E8%A7%A3-HashCode.html","excerpt":"Java 中所有的类都继承自 Object 类，Object 类中有个返回 hashCode 的本地方法。 1public native int hashCode(); 在文档的注释中很清楚的说明了 hashCode 的作用，和它应该满足的一些要求。","text":"Java 中所有的类都继承自 Object 类，Object 类中有个返回 hashCode 的本地方法。 1public native int hashCode(); 在文档的注释中很清楚的说明了 hashCode 的作用，和它应该满足的一些要求。 作用：给一个对象返回一个 hashCode 值，这个值在 hash table 的数据结构中有重要的作用。例如，确定放置在 hash table 哪个索引位置，hash 冲突的频率。 要求： 同一个 Java 对象，在程序运行的整个生命周期中。该对象返回的 hashCode 应该相同。 使用 equals 方法，判断为两个相等的对象，其返回的 hashCode 应该相同。 使用 equals 方法，判断为两个不相同的对象，其返回的 hashCode 应该不相同。 通常的 hashCode 生成方法是将对象的内存地址转换成一个整型数，这样就能为不同的对象返回一个不一样的 hashCode。但是这种方法不能满足上面的第二个条件，所以这种实现也不是 Java 语言所必须的实现方法。 在 String 中的实现String 类也是继承自 Object 类，它重写了 hashCode() 方法。 123456789101112131415/** Cache the hash code for the string */private int hash; // Default to 0public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 在 String 中计算的 hashCode 会存储在 hash 变量中，并且只会计算一次。因为 String 是 final 的，并且一个 String 对象被初始化后无法修改，所以它的 hashCode 不会变化。 for 循环计算 hashCode 的方法是根据以下式子：s[0]*31^(n-1) + s[1]*31^(n-2) + … + s[n-1]。 使用 31 的原因31 是一个质数（Prime number），质数也称为素数。质数是大于 1 的自然数，且只能被 1 和其本身整除。 选择 31 的原因大概有以下几点： 一个数乘质数后的结果，只能被 1 、质数、乘数还有结果本身整除。计算 hashCode 选择一个优质质数可以降低 hash 的冲突率。 31 （2 &lt;&lt; 5 - 1），在计算的过程中可以被 JVM 优化。 相信第二点很多同学都能够理解，现在解释一下第一点。 我们列举一下 100 以内左右的质数：2，3，5，7，11，13，17，19，23，29，31，37，41，43，47，53，59，61，67，71，73，79，83，89，97。 从上面的质数中选择三个小中大质数：2，31，97。分析公式 s[0]*31^(n-1) + s[1]*31^(n-2) + … + s[n-1] 中的每一项，都是一个数乘质数的平方项。所以我们计算一下每个质数的 n 次方，我们选择 n = 5。那么结果如下： 质数 结果 2 2^5 = 32 31 31^5 = 28,629,151 97 97^5 = 8,587,340,257 可以看到通过质数 2 计算后的 hashCode 值是一个比较小的值，而通过质数 97 计算后的 hashCode 是一个比较大的值，而 31 比较适中。 我们可以认为 hashCode 的范围若太小，可能会增加 hash 冲突的概率。而计算 hashCode 的计算乘子太大容易导致整型数的溢出（这里并不是说选择 31 不会导致溢出，是指一个导致溢出的速率），从而也会导致 hash 冲突的概率。31 可以有效的减轻这两点。 更详细的内容可以看一下 stackoverflow 上面的这个问题：Why does Java’s hashCode() in String use 31 as a multiplier? 设计 hashCode 算法根据《Effective Java》第二版中的第 9 条，对于我们自己编写的类，覆盖 equals 方法时需要覆盖 hashCode 方法。原因在前面说过。 那么如何设计一个 hashCode 算法，书中设计了一个算法： 把某个非 0 的常数值，比如 17，保存在一个名为 result 的 int 类型的变量中。 对于对象中的每个域，做如下操作： 为该域计算 int 类型的哈希值 c ： 如果该域是 boolean 类型，则计算 (f?1:0) 如果该域是 byte、char、short 或者 int 类型，则计算 (int)f 如果该域是 long 类型，则计算 (int)(f^(f&gt;&gt;&gt;32)) 如果该域是 float 类型，则计算 Float.floatToIntBits(f) 如果该域是 double 类型，则计算 Double.doubleToLongBits(f)，然后重复第三个步骤。 如果该域是一个对象引用，并且该类的 equals 方法通过递归调用 equals 方法来比较这个域，同样为这个域递归的调用 hashCode，如果这个域为 null，则返回0。 如果该域是数组，则要把每一个元素当作单独的域来处理，递归的运用上述规则，如果数组域中的每个元素都很重要，那么可以使用 Arrays.hashCode 方法。 按照公式 result = 31 * result + c，把上面步骤 2.1 中计算得到的散列码 c 合并到 result 中。 返回 result 参考科普：为什么 String hashCode 方法选择数字31作为乘子 Why does Java’s hashCode() in String use 31 as a multiplier? 《Effective Java》"},{"title":"Linux IO 原理","date":"2022-08-20T05:46:18.006Z","updated":"2018-09-19T08:40:37.000Z","comments":true,"path":"back_posts/old_posts/Linux/Linux-IO-原理.html","permalink":"https://orechou.github.io/back_posts/old_posts/Linux/Linux-IO-%E5%8E%9F%E7%90%86.html","excerpt":"在上一篇文章中主要介绍了 Java NIO 相关基础和 Reactor 模式，NIO 的实现在不同的平台上依赖于操作系统所提供的系统调用。下面将介绍 NIO 在 Linux 平台上的底层实现原理。","text":"在上一篇文章中主要介绍了 Java NIO 相关基础和 Reactor 模式，NIO 的实现在不同的平台上依赖于操作系统所提供的系统调用。下面将介绍 NIO 在 Linux 平台上的底层实现原理。 操作系统基础首先了解一下类 Unix 操作系统体系结构与基本概念。 类 Unix 的体系结构 操作系统内核（kernel）的本质是一个软件：可以控制计算机的硬件资源，并提供上层应用程序运行的环境。内核拥有可以访问受保护的空间和底层硬件设备的权限。 用户所编写的程序不能直接去访问内核代码，需要通过接口，这些接口被设计为系统调用（system call）。公用函数库建立在系统调用之上，应用程序即可以直接使用公用函数库，也可以使用系统调用。例如 fread 是 C 标准库函数，read 是系统调用。 Shell 是一个特殊的应用程序，为运行其他应用程序提供了一个接口。最外层的是应用程序。 用户态 &amp; 内核态类 Unix 的体系架构可以分为用户态和内核态。操作系统将虚拟空间划分成用户空间和内核空间两个部分，并划分了特权级，Intel x86 架构的 cpu 一共有 0～3 四个特权级，数字越小，特权越高，在 Unix/Linux 中指使用了 0 和 3 两个特权级，分别表示内核态和用户态。 用户应用程序一般在用户态下运行，当需要操作系统介入时，就会切换到用户态。用户态到内核态的切换方式： 系统调用：用户态进程主动通过系统调用申请操作系统提供的服务程序。（系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断） 异常：当CPU正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常。 外设中断：当外围设备完成用户的请求操作后，会像CPU发出中断信号，此时，CPU就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。 Linux 进程地址空间下图是 Linux 进程的内存地址空间，以 32 位的操作系统为例。 地址空间分成了两部分，用户空间（0x00000000 ~ 0xBFFFFFFF）和内核空间（0xC0000000 ~ 0xFFFFFFFF）。不同的进程用户空间是私有的。内核空间是持续存在的，并且是共享的，所有进程中都映射到同样的物理内存，内核代码和数据总是可寻址的，随时准备处理中断和系统调用。 用户空间的由如下的内容组成： Text Segmet（ELF，文本段）：程序代码在内存中的映射，存放二进制代码。 Data Segmet（初始化过的数据段）：存放程序运行时已经初始化赋值的静态变量数据（被程序员初始化）。 BSS Segment（未初始化的数据段）：存放程序运行时未被初始化的静态变量数据。 Heap：存储动态内存分配，需要程序员手工分配，手工释放。与数据结构中的堆是两回事，分配方式类似于链表。 栈：当有方法的调用的时候会产生一个方法栈帧进行压栈。栈帧中存储局部、临时变量，函数调用，存储函数的返回指针，用于控制函数的调用和返回。栈帧在函数程序块开始时自动分配内存，结束时自动释放内存。 数据读写Linux 的数据读写有两种方式，传统的读写与零拷贝技术。传统的读写数据需要复制两次，零拷贝技术只需要复制一次。下面以文件读写为例，简要说明一下传统的读写。 读数据 当应用程序需要读写磁盘上的数据时，会使用内核的系统调用。数据首先通过 DMA（Direct Memory Access）将磁盘上的数据复制到内核的缓冲区中，然后应用程序再从内核缓冲区中将数据复制到应用缓冲区。 写数据 当应用程序需要将数据传送给另外一个客户端时，也会使用内核的系统调用。应用程序首先将数据写入到内核的 socket 缓冲区中，然后再通过 DMA 将数据发给 socket 连接的另一端。 DMA copy。这是一种通过硬件实现的数据传输机制。简单的说，就是不在CPU的参与下完成数据的传输。 CPU copy 。 相比DMA而言，copy的过程需要用到cpu 寄存器等,速度较慢。 零拷贝技术考虑一个应用场景，服务器程序经常需要将一些数据通过网络传输给另外一个程序。 传统方式传统的处理方式是先将数据从磁盘上读出来，然后再将数据通过 socket 写出去。这会使用两个函数： 12File.read(fileDesc, buf, len);Socket.send(socket, buf, len); 流程图如下： 由图中我们可以看出数据一共被复制了 4 次： 磁盘复制到内核 Read Buffer Read Buffer 复制到用户空间 Buffer 用户空间 Buffer 复制到 Socket Buffer Socket Buffer 复制到 NIC Buffer 注：NIC（network interface card） 在整个过程中数据一共复制了 4 次，用户态-内核态切换了 4 次。Read 两次，Send 两次。 零拷贝零拷贝技术让整个数据复制的过程中不需要用户的参与，即数据传输直接在内核中完成，不需要复制到用户空间的缓存中。 12// out_fd: socket 或者 file 都可以用 fd 表示ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 流程图如下： 数据从磁盘复制到内核 Read Buffer 中后，直接复制到 Socket Buffer 中。 所以整个过程中数据只需要复制 3 次，用户态-内核态切换了两次。 Linux IO 模式对于一次 IO 访问（以 read 举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready)，即数据复制到内核的缓冲区。 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)，内核缓冲区到用户空间的缓冲区。 linux 一共有下面五种 IO 模式： 阻塞 IO（blocking IO） 非阻塞 IO（nonblocking IO） IO 多路复用（ IO multiplexing） 信号驱动 IO（ signal driven IO） 异步 IO（asynchronous IO） 在实际的使用中，信号驱动 IO 并不常用。 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 Unix、Linux这样的操作系统。 阻塞 IO（Blocking IO）在 Linux 中所有的 IO 默认都是阻塞模式，阻塞 IO 的流程如下： recvfrom 是从 socket 接受数据的系统调用。 1234ssize_t recv(int sockfd, void *buf, size_t len, int flags);ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags); 当使用这个系统调用的时候，内核会介入完成两部分操作： 准备数据，这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要时间的，不管是复制磁盘文件中的数据，还是接收 socket 传输的数据。而等待这一个过程，用户进程会将自己阻塞。 当内核中的缓冲区已经将数据复制好之后，就会将数据从内核中的缓冲区复制到用户空间中的缓冲区。复制完成之后返回结果，用户进程继续执行。 可以看到，用户进程在 IO 操作的两个阶段中都是被阻塞的。 非阻塞 IO（NonBlocking IO）非阻塞 IO 的流程如下： 在 Linux 中，可以通过以下三种方式将 IO 设置为非阻塞 IO： 123456789// 1.创建socket的时候，指定 socket 是异步的，在 type 的参数中设置 SOCK_NONBLOCK 标志即可。int socket(int domain, int type, int protocol);int s = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, IPPROTO_TCP);// 2.使用fcntl函数。fcntl(sockfd, F_SETFL, fcntl(sockfd, F_GETFL, 0) | O_NONBLOCK);// 3.使用ioctl函数。1:非阻塞 0:阻塞。ioctl(sockfd, FIONBIO, 1); 非阻塞 IO 与阻塞 IO 的区别在于，在数据准备的阶段，用户进程不需要等待该阶段完成。用户进程会不断地去询问这个过程是否完成，若未完成用户进程也会收到一个回复，若已完成则开始第二阶段数据的复制。 多路复用 IO（IO Multiplexing）多路复用 IO 也被称为事件驱动 IO，流程如下： IO 多路复用通过 select、poll、epoll 实现。IO 多路复用优点在于单个 process 可以同时处理多个网络连接的IO。它的基本原理就是 select，poll，epoll 函数会不断的轮询所负责的所有连接的 socket，当某个 socket 有数据到达，就通知用户进程。 当用户调用了 select 后，用户进程需要等待 select 函数的返回，即有 socket 的数据就绪。在 Java 中的 Selector 提供了 select 方法和 selectNow 方法，前者需要等待有就绪的读写事件，而后者可以直接返回。 123456// allow a program to monitor multiple file descriptors, waiting until one or more of the file descriptors become &quot;ready&quot; for some class of I/O operationint select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);// wait for some event on a file descriptorint poll(struct pollfd *fds, nfds_t nfds, int timeout); 异步 IO（Asynchronous IO）异步 IO 是真正的异步非阻塞 IO，流程如下： 用户进程发起 read 操作之后，就立刻返回开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 对比在 non-blocking IO 中，虽然进程大部分时间都不会被 block，但是它仍然要求进程去主动的 check，并且当数据准备完成以后，也需要进程主动的再次调用 recvfrom 来将数据拷贝到用户内存。而 asynchronous IO 则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 多路复用的原理前面说到 Linux 实现多路复用是通过 select、poll、epoll 函数实现的。IO 多路复用是一种机制，这种机制提供了单个进程监听多个 IO 的能力。一个进程可以通过监听多个文件描述符，一旦某个描述符的某种事件就绪（读、写就绪），就通知程序进程读写操作。 selectselect 函数的定义如下： 12// allow a program to monitor multiple file descriptors, waiting until one or more of the file descriptors become &quot;ready&quot; for some class of I/O operationint select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 关注了三种描述符，readfds、writefds、exceptfds，分别是读、写和发生异常的。可以通过设置 timeout 来指定等待事件，当将 timeout 设置为 null 时，select 会立即返回。函数返回值是就绪文件描述符的数量，0 表示timeout，-1 表示出错。函数返回后要通过遍历的方式找到就绪的fd。 select 采用的是轮询的方式监听所有的文件描述符，单个进程能够监听的文件描述符的数量有最大限制，在Unix下通常为256，Linux 下通常为1024，可以通过修改 /sys/types.h 头文件中的 FD_SETSIZE 值然后重新编译内核进行修改。 缺点： 可以监听的文件描述符有限。 以轮询的方式进行检查是否就绪效率低。 每次检查是否就绪要将所有需要被检查的 fd 的数据结构复制到内核，完成后修改这个数据结构并返回给程序，从用户空间到内核空间的来回拷贝将占用大量 CPU。 优点：在所有的平台上都支持 pollpoll 函数的定义如下： 12345678// wait for some event on a file descriptorint poll (struct pollfd *fds, unsigned int nfds, int timeout);struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */&#125;; poll 也是以轮询的方式进行监听，但与 select 不同的是 poll 抛弃了用位图存储文件描述符的方式，而使用 pollfd。需要监听什么事件，只需要初始化 events 即可，当 poll 函数返回时 revents 被设定为实际发生的事件。 缺点： 以论文的方式进行检查是否就绪效率低。 每次检查是否就绪要将所有需要被检查的 fd 的数据结构复制到内核，完成后修改这个数据结构并返回给程序，从用户空间到内核空间的来回拷贝将占用大量 CPU。 优点： 监听没有最大数量的限制。 select 的函数返回值是三个集合中就绪态的 fd 的数量之和，因此如果同一个 fd 在不止一个集合中同时被指定，且对于多个 I/O 事件都处于就绪态的话就会被统计多次，而 poll 的返回值是不重复的。 select 和 poll 都是需要通过遍历所有的文件描述符来获取已经就绪的事件，但是同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epollepoll 是在2.6内核中提出的，是之前的 select 和 poll 的增强版本。epoll更加灵活，没有描述符限制。epoll 使用一个文件描述符管理多个描述符，其他的文件描述符注册到这个描述符上，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次。 epoll 有三个函数： 1234567891011// 创建一个 epoll 的句柄（实例），size 用来告诉内核这个监听的数目一共有多大int epoll_create(int size)；// 修改 epfd 所代表的 epoll 实例的兴趣列表，即可以向 epoll 实例添加、删除、修改 fd，并设定感兴趣的事件int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；// 返回 epoll 实例中处于就绪态的 fd 信息int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);struct epoll_event &#123; uint32_t events; /*指定了待检查的fd所感兴趣的事件集合，用位掩码表示*/ epoll_data_t data; /*用来在就绪后给调用进程传递信息*/&#125;; 步骤epoll_create调用该函数创建一个句柄（称为 epoll 句柄，epfd），指定的 size 用来告诉内核这个监听的数目的大小。size 不是限制 epoll 监听描述符的最大数量，而是内核对内存使用大小的一个建议。 epoll 句柄是一个文件描述符，它会占用一个 fd 值。在 Linux 中通过 /proc/pid/fd/ 可看到该值。所以在使用 epoll 的程序中，使用完之后应用 close 方法关闭 epoll，否则可能会导致 fd 被消耗殆尽。 epoll_ctl首先将一下函数中各个参数的意义： 参数 含义 epfd 调用 epoll_create 函数产生的 epoll 句柄 op 用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对 fd 的监听事件。 fd 监听的文件描述符 fd epoll_event 监听 fd 的事件 函数对指定的文件描述符 fd 执行指定的 op 操作。 event 集合如何： 123456EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 epoll_wait参数及意义如下： 参数 含义 epfd 调用 epoll_create 函数产生的 epoll 句柄 events 从内核得到的事件集合 maxevents 告诉内核这个 events 有多大，该值不能大于创建 epoll_create() 时的 size timeout 超时时间 该函数等待 epfd 上的 IO 事件。 工作模式epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger）。 参考资料[Linux IO模式及 select、poll、epoll详解]"},{"title":"Linux 命令","date":"2022-08-20T05:46:18.006Z","updated":"2018-09-27T12:33:48.000Z","comments":true,"path":"back_posts/old_posts/Linux/Linux-命令.html","permalink":"https://orechou.github.io/back_posts/old_posts/Linux/Linux-%E5%91%BD%E4%BB%A4.html","excerpt":"Linux 命令时刻都会用到，除了 ls、cd、chmod 等，常用的可以分成两类，系统性能监控相关的命令和文本处理相关的命令。","text":"Linux 命令时刻都会用到，除了 ls、cd、chmod 等，常用的可以分成两类，系统性能监控相关的命令和文本处理相关的命令。 常用系统相关的命令： 命令 说明 top 能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器 ps 用于显示当前进程的状态。 iostat 对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。 free 显示系统使用和空闲的内存情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。共享内存将被忽略。 vmstat 可对操作系统的虚拟内存、进程、CPU 活动进行监控。它是对系统的整体情况进行统计，不足之处是无法对某个进程进行深入分析。 lsof （list open files）列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。用于查看你进程开打的文件，打开文件的进程，进程打开的端口( TCP、UDP )。 netstat 显示与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，一般用于检验本机各端口的网络连接情况。 常用文件、文本处理相关的命令： 命令 说明 cat、more、less、head、tail 不用多说，查看文件。 find 在目录结构中搜索文件，并执行指定的操作。 grep 主要用于文本内容的查找，它支持正则表达式查找。 cut 可以按列来切分文件，可用于处理使用固定宽度字段的文件。 sed （stream editor）流编辑器，常用的用法是进行文本替换。 awk 可进行高级文本处理。通常由 3 个部分组成：BEGIN、END 和带模式匹配选项的公共语句块，但是三个部分都是可选的。awk 以逐行的形式处理文件。（1）首先执行 BEGIN 语句块中的语句；（2）接着从文件或 stdin 中读取一行，如果能改匹配 pattern，则执行随后的语句块中的语句。重复该过程直到数据读取结束。（3）当读到输入数据流的结束之后，执行 END 语句块。 which 查找某个命令的位置。 whereis 跟 which 类似，还能改打印出其对应的命令手册以及源代码的路径。 一些参数： -f：指定字符"},{"title":"Socket 入门","date":"2022-08-20T05:46:18.120Z","updated":"2018-09-27T13:08:52.000Z","comments":true,"path":"back_posts/old_posts/Linux/Socket-入门.html","permalink":"https://orechou.github.io/back_posts/old_posts/Linux/Socket-%E5%85%A5%E9%97%A8.html","excerpt":"Socket 英文有插座的意思，可以想象端口就是插座上的口，端口不能被其他进程占用。想象 Socket 类似于操作某个 IP 地址上的某个端口以达到点对点通信的目的，需要绑定到某个具体的进程和端口。 Socket 原本代表的是 Unix 上的原始套接字（RawSocket）用于描述文件的内存镜像，但因为 Unix 的设计哲学是“一切都是文件”，所以后来的网络版的进程间通信也就被冠名为文件描述符。本质上讲 Socket 是 Unix 系统设计的一种思想。 Socket 没有层的概念，可以把它看成是对 TCP/IP 协议簇的一个门面模式的封装。可以让编程变得更加简单。","text":"Socket 英文有插座的意思，可以想象端口就是插座上的口，端口不能被其他进程占用。想象 Socket 类似于操作某个 IP 地址上的某个端口以达到点对点通信的目的，需要绑定到某个具体的进程和端口。 Socket 原本代表的是 Unix 上的原始套接字（RawSocket）用于描述文件的内存镜像，但因为 Unix 的设计哲学是“一切都是文件”，所以后来的网络版的进程间通信也就被冠名为文件描述符。本质上讲 Socket 是 Unix 系统设计的一种思想。 Socket 没有层的概念，可以把它看成是对 TCP/IP 协议簇的一个门面模式的封装。可以让编程变得更加简单。 基础文件描述符文件描述符在形式上是一个非负整数。实际上，它是一个索引值，操作系统为每个运行的进程维护一张单独的文件描述符表。当程序打开一个现有文件或者创建一个新文件时，系统把一个指向此该文件内部数据结构的指针写入文件描述符表，并把该表的索引值返回给调用者 。应用程序只需记住这个描述符，并在以后操作该文件时使用它。操作系统把该描述符作为索引，用来访问进程描述符表，通过指针找到保存该文件所有的信息的数据结构。 系统创建一个 socket 连接，也是会打开一个 Socket 文件描述符。 函数Socket 通信双方的函数如下： socket1int socket(int domain, int type, int protocol); socket() 函数对应于普通文件的打开操作。普通文件的打开操作返回一个文件描述字，而 socket() 用于创建一个 socket 描述符（socket descriptor），它唯一标识一个socket。 打开一个网络通讯端口，如果成功的话，返回一个文件描述符，应用程序可以像读写文件一样用 read/write 在网络上收发数据，如果 socket() 调用出错则返回-1。 bind1int bind(int sockfd, const struct sockaddr *myaddr, socklen_t addrlen); 服务器需要调用 bind 绑定一个固定的网络地址和端口号。服务器程序所监听的网络地址和端口号通常是固定不变的，客户端程序得知服务器程序的地址和端口号后就可以向服务器发起连接。 bind() 的作用是将参数 sockfd 和 myaddr 绑定在一起，使 sockfd 这个用于网络通讯的文件描述符监听 myaddr 所描述的地址和端口号。 listen1int listen(int sockfd, int backlog);// backlog取值0~5. listen() 声明 sockfd 处于监听状态，并且最多允许有 backlog 个客户端处于连接待状态，如果接收到更多的连接请求就忽略。 listen() 成功返回 0，失败返回 -1。 accept1int accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen); 三方握手完成后，服务器调用 accept() 接受连接，如果服务器调用 accept() 时还没有客户端的连接请求，就阻塞等待直到有客户端连接上来。"},{"title":"计算机网络：传输层协议","date":"2022-08-20T05:46:17.914Z","updated":"2018-09-26T09:25:50.000Z","comments":true,"path":"back_posts/old_posts/Networks/计算机网络：传输层协议.html","permalink":"https://orechou.github.io/back_posts/old_posts/Networks/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%9A%E4%BC%A0%E8%BE%93%E5%B1%82%E5%8D%8F%E8%AE%AE.html","excerpt":"在计算机网络的分层协议的体系结构中，不管是 OSI 体系结构、五层体系结构还是 TCP/IP 体系结构，它们都有传输层。传输层提供了进程间的通用数据传输服务。应用层的协议很多，定义通用的传输层协议就很重要，能够支持不断增多的应用层协议。 如下图所示，TCP/IP 协议簇呈现沙漏的形状，TCP 和 IP 协议起到了举足轻重的作用。网络层 IP 协议提供了尽最大努力交付的功能，而要保证数据的正确传输，需要 TCP 协议来做这个事情。 传输层一共提供了两种协议： 传输控制协议 用户数据报协议","text":"在计算机网络的分层协议的体系结构中，不管是 OSI 体系结构、五层体系结构还是 TCP/IP 体系结构，它们都有传输层。传输层提供了进程间的通用数据传输服务。应用层的协议很多，定义通用的传输层协议就很重要，能够支持不断增多的应用层协议。 如下图所示，TCP/IP 协议簇呈现沙漏的形状，TCP 和 IP 协议起到了举足轻重的作用。网络层 IP 协议提供了尽最大努力交付的功能，而要保证数据的正确传输，需要 TCP 协议来做这个事情。 传输层一共提供了两种协议： 传输控制协议 用户数据报协议 UDP用户数据报协议 UDP （User Datagram Protocol）是无连接的协议，它提供尽最大可能的交付，面向报文。没有流量控制和拥塞控制，支持一对一、一对多、多对一和多对多的通信模式。 首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。在计算检验和时，要在 UDP 用户数据报之前增加 12 个字节的伪首部。“伪首部”并不是用户数据报真正的首部，只是在计算检验和时，临时添加在 UDP 数据报前面，得到一个临时 UDP 数据报。伪首部既不向下传送也不向上递交，仅仅是为了计算检验和。 根据 UDP 报文格式可以看出它没有像 TCP 那样的序号，所以一份 UDP 报文就是一份数据。而 IP 数据报最大长度为 65535 字节，去除 20 个 IP 首部和 8 个字节的 UDP 首部，UDP 数据报中数据的最大长度为 65507 字节。所以 UDP 是有最大报文长度限制的。 适用场景首先明确一下 UDP 的特点，无连接、多种通信模式和具有最大报文长度的限制。无连接的具有速度快的优点，缺点是容易丢失报文，且报文一旦丢失就再也无法接收到了。所以要求实时性并且接受一定程度的数据丢失的应用场景可以使用 UDP，例如实时通讯、在线视频等。另外 UDP 支持一对多和多对多的通信模式，也可以使用在多播通信的场景中。 TCP传输控制协议 TCP （Transmission Control Protocol）是面向连接的协议，它提供了可靠的交付，面向字节流。具有流量控制和拥塞控制的功能，提供一对一全双工的通信模式。 首部格式 TCP 的首部内容比 UDP 要多一些。首先有 20 字节的固定长度的内容： 源端口 &amp; 目的端口 序号 seq：当前报文的序号。TCP 是面向字节流的。比如第一个报文，会给第一个字节编上一个序号，例如 seq = 300。若报文的携带的字节流的长度为 100，那么下一个报文的 seq 则为 400。 确认号 ack：期望收到的下一个报文的序号。例如接收方收到序号为 300，长度为 100 的报文。那么他返回的 ack 为 400。 数据偏移：指的是数据部分距离报文起始处的偏移量，也可以当做是报文首部的长度。 确认 ACK：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 三次握手TCP 建立连接要经过三次握手，假设存在客户端 A 和服务端 B，它们握手的整个过程如下： 过程 服务端 B 处于 LISTEN（监听）状态，等待客户的连接请求。 客户端 A 向服务端 B 发送连接请求报文，SYN=1，ACK=0，并选择一个序列号 x。 服务端 B 接收到请求报文，若同意建立连接，则向客户端 A 发送确认报文。并且选择一个报文序列号 y。 A 收到 B 的确认报文之后，向 B 发送确认报文，确认号为 y+1, 序列号为 x+1。 B 收到 A 的确认报文后，连接建立。 三次握手原因三次握手在于为了防止失效的请求连接到达服务器，让服务器以为客户端又申请连接，从而错误地打开了连接。 因为网络的环境是不确定的，当请求方客户端发送的请求连接报文在网络中滞留，客户端在等待超时之后会继续重新发送请求连接。若不进行三次握手，当客户端后面的请求连接建立通讯完成之后，最开始发送的滞留报文被传递给了服务端。服务端会以为客户端申请了新的连接，从而打开连接占用资源。若使用三次握手，服务端就会去确认这个请求连接是否是客户端要发起建立的，避免错误地打开连接。 半连接和全连接队列TCP 请求中有两个队列，半连接队列（syns queue）和全连接队列，如下图所示： 在握手的第一步中，服务端接收到连接的请求后，会将相关的信息存储到半连接队列中。 在握手的最后一步，服务端接收到客户端的 ACK 后，会将相关信息从半连接队列中取出存入到全连接队列中。 syn floods 攻击就是针对 TCP 半连接队列，导致该队列满，从而无法接受其他正常的连接请求。 当服务端接收到连接请求时，发现全连接队列满已满，那么服务端会过一段时间才会发送 FIN ACK 报文给请求方。 队列长度可以通过 backlog 参数进行设置。我在这里理解的长度是半连接队列 + 全连接队列的参考长度。 1int listen(int sockfd, int backlog); 四次挥手TCP 断开连接需要进行四次挥手，整个挥手的过程如下： 过程假设客户端 A 首先发送完数据。 客户端 A 会发送终止报文，FIN=1，序列号为 u。此刻 A 会进入FIN-WAIT-1 的状态。 服务端 B 收到终止报文后发送确认报文，此时 TCP 处于半关闭状态，B 可以向 A 发送数据但 A 不能向 B 发送数据。B 进入 CLOSE-WAIT 状态。 客户端 A 收到 B 的确认报文之后，进入 FIN-WAIT-2 的状态，并接受 B 未传递完的数据。 当 B 发送完数据之后发出终止报文，进入 LAST-ACK 状态。 A 接收到 B 的终止报文后，发出确认报文。并进入 TIME-WAIT 状态。 B 收到确认报文后关闭连接，若 A 在 TIME-WAIT 状态下等待 2MSL 时间未收到 B 的重传报文，则关闭连接。 四次挥手的原因进行四次挥手的原因在于，TCP 的通讯是全双工的，通信的双方都需要发送一次终止连接的报文，而接受的另一方需要发送确认报文才可以告知对面我已经收到断开报文的请求。 等待 2MSL 原因MSL（Maximum Segmet Lifetime）指的是报文在网络中能够存在的最长时间。 客户端处于 FIN-WAIT-2 状态时收到服务端的终止报文后悔进入 TIME-WAIT 状态，并等待两个 MSL，这样做的原因有两个： 确保最后一个确认报文能够被对方接受。若 B 未接收到 A 发送的确认报文，那么 B 就会重新发送终止报文给 A。A 若再次收到终止报文，那么就知道 B 并没有接收到 A 的确认报文，就能够继续发送。 等待的这段时间内可以确认这次连接中所发送的所有报文都会在网络中消失，当建立下一次连接的时候，可以确定不会接收到上一次连接中所发送的报文。 可靠传输TCP 相比与 UDP 最大的不同就是，TCP 能够保证传输是可靠的。可靠传输通过超时重传、校验和、流量控制、拥塞控制来实现。 超时重传发送方发送一个报文后，会启动一个定时器。若已个已经发送的报文在超时时间内没有收到确认，那么就重传这一个报文。 校验和TCP 报文头部中有部分是根据首部和数据计算的校验和，接收方可以计算校验和然后这个校验和相比较，判断数据在整个传输的过程中是否发生了变化。若接收方接收到的校验和有差错，那么就丢弃该报文，让发送方再次重传。 控制流量控制和拥塞控制都要基于滑动窗口的机制。 滑动窗口窗口属于缓存，用来暂存字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 流量控制流量控制是为了控制发送方发送速率，保证接收方来得及接收。接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 拥塞控制TCP 拥塞控制目的在于当网络中出现了拥塞，若发送方继续重传报文和发送新报文，则会导致网络中的拥塞程度变得更高。所以当网络出现拥塞的时候，需要控制发送方的速率。 TCP 通过四个方法来进行拥塞控制：满开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。 慢开始 &amp; 拥塞避免发送的最初执行慢开始，令 cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 … 注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。 如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始。 快重传 &amp; 快恢复在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M2，则 M3 丢失，立即重传 M3。 在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。 慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。"}],"posts":[{"title":"【Weekly】No.3 带娃 2","slug":"weekly/No.3 带娃 2","date":"2022-11-06T14:00:00.000Z","updated":"2022-11-07T08:57:23.140Z","comments":true,"path":"2022/11/06/weekly/No.3 带娃 2/","link":"","permalink":"https://orechou.github.io/2022/11/06/weekly/No.3%20%E5%B8%A6%E5%A8%83%202/","excerpt":"","text":"本周的主题就是带娃。 TechnologyBackend交易系统 SpringCloud 实现WrapExchange Web3Views“Web3的特点并不是用户掌握自己的数据。回顾下90年代Web 1.0的互联网愿景：信息技术使世界变成了地球村，可以与任何人实时沟通；每个人都可以平等地获得知识和信息，教育更平等，个人更文明；世界各国各民族会变得更加容易沟通，更和谐。看看哪一条实现了？以facebook为代表的Web 2.0愿景：连接每一个人（不好意思只能在我的平台里连接）；自由讨论与交流（不好意思为了高效推广告你看到的是推送算法让你看到的）；随时发布与获取信息（不好意思只能在平台发，平台外面连搜索引擎都搜不到）。炒项目可以靠情怀，但真信了就会变韭菜。”———廖雪峰，知乎回答这是廖雪峰在知乎上发表的关于 Web3 不一样的看法，观点也值得思考一下。 Articles当我们在谈论去中心化身份（DID）时，我们在谈论什么","categories":[],"tags":[{"name":"weekly","slug":"weekly","permalink":"https://orechou.github.io/tags/weekly/"}]},{"title":"【Weekly】No.2 整理 & 带娃","slug":"weekly/No.2 整理 & 带娃","date":"2022-10-31T14:00:00.000Z","updated":"2022-11-01T07:37:22.812Z","comments":true,"path":"2022/10/31/weekly/No.2 整理 & 带娃/","link":"","permalink":"https://orechou.github.io/2022/10/31/weekly/No.2%20%E6%95%B4%E7%90%86%20&%20%E5%B8%A6%E5%A8%83/","excerpt":"","text":"本周开始休陪产假，连续 15 天。这两周第一个计划是适配娃的节奏，如果可能的话降低她的起夜次数；第二个计划是养成一个记录的习惯，记录技术、生活等内容。以下内容记录于：20221024 ~ 20221030 TechnologyBackendJava 线程池异常处理ThreadPoolExcutor#execute 执行 task 会异常会直接抛出来，ThreadPoolExcutor#submit 则不会，因为其会返回一个 Future，异常在其执行的内部逻辑中已经被捕获了。处理异常一共有三种方式：1 try-catch自不必细说。2 Thread.setDefaultUncaughtExceptionHandler 123456789101112131415161718192021222324252627282930313233public class ThreadPoolException &#123; public static void main(String[] args) throws InterruptedException &#123; //1.实现一个自己的线程池工厂 ThreadFactory factory = (Runnable r) -&gt; &#123; //创建一个线程 Thread t = new Thread(r); //给创建的线程设置UncaughtExceptionHandler对象 里面实现异常的默认逻辑 t.setDefaultUncaughtExceptionHandler((Thread thread1, Throwable e) -&gt; &#123; System.out.println(&quot;线程工厂设置的exceptionHandler&quot; + e.getMessage()); &#125;); return t; &#125;; //2.创建一个自己定义的线程池，使用自己定义的线程工厂 ExecutorService executorService = new ThreadPoolExecutor( 1, 1, 0, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(10), factory); // submit无提示 executorService.submit(new task()); Thread.sleep(1000); System.out.println(&quot;==================为检验打印结果，1秒后执行execute方法&quot;); // execute 方法被线程工厂factory 的UncaughtExceptionHandler捕捉到异常 executorService.execute(new task()); &#125; &#125; 3 重写afterExecute这种方式可以防止异常 submit 异常被吞掉的情况。 123456789101112131415161718192021222324252627282930313233343536373839public class ThreadPoolException3 &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; //1.创建一个自己定义的线程池 ExecutorService executorService = new ThreadPoolExecutor( 2, 3, 0, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(10) ) &#123; //重写afterExecute方法 @Override protected void afterExecute(Runnable r, Throwable t) &#123; //这个是excute提交的时候 if (t != null) &#123; System.out.println(&quot;afterExecute里面获取到excute提交的异常信息，处理异常&quot; + t.getMessage()); &#125; //如果r的实际类型是FutureTask 那么是submit提交的，所以可以在里面get到异常 if (r instanceof FutureTask) &#123; try &#123; Future&lt;?&gt; future = (Future&lt;?&gt;) r; //get获取异常 future.get(); &#125; catch (Exception e) &#123; System.out.println(&quot;afterExecute里面获取到submit提交的异常信息，处理异常&quot; + e); &#125; &#125; &#125; &#125;; //当线程池抛出异常后 execute executorService.execute(new task()); //当线程池抛出异常后 submit executorService.submit(new task()); &#125; &#125; Java线程池实现原理及其在美团业务中的实践线程池中异常处理 Web3Aptoshttps://aptoslabs.com/community LifeMovies《功夫熊猫》豆瓣链接：功夫熊猫偶然间在 B 站刷到《功夫熊猫》电影里面的一个片段，就很想再重温相关的三部电影。趁着罗湖与光明的往返地铁，再加上一个傍晚，我把相关的电影看了一遍。08 年的电影，那时的国际环境没这么紧张，国外还有专研中国传统文化元素的情绪。似乎过去的内容是这么的美好，也或许那时的我还是小孩只是感受不到生活的艰辛而已。 Family整理 &amp; 带娃从 2019 年来到深圳，总共搬了两次家。前两次都在福田区上沙，这次搬到了光明区公明。这周花了很大的力气把三年来两个人囤积的物品在新家整理干净，还有两只猫的东西没有整理完。关于带娃的部分，可瑷到了一月龄之后很容易肠胀气，之前是安安静静吃奶，现在有时会因为胀气哭闹。还是比较磨人的。有了小孩之后，相当一部分精力都会分给小孩。很难想象下周要去上班了该怎么办。","categories":[],"tags":[{"name":"weekly","slug":"weekly","permalink":"https://orechou.github.io/tags/weekly/"}]},{"title":"【Weekly】No.1 开始周记","slug":"weekly/No.1 开始周记","date":"2022-10-23T14:00:00.000Z","updated":"2022-10-24T08:49:54.649Z","comments":true,"path":"2022/10/23/weekly/No.1 开始周记/","link":"","permalink":"https://orechou.github.io/2022/10/23/weekly/No.1%20%E5%BC%80%E5%A7%8B%E5%91%A8%E8%AE%B0/","excerpt":"","text":"这个周记记录一下每周的积累。题材不限，可包含各种技术、资源、新闻等。本周结束后开始休产假，休到 11 月 7 号。 TechnologyBackendMySQL 降序索引降序索引是以降序存储键值的索引，在使用中可以用 DESC 关键字指定。 12345CREATE TABLE t( a INT NOT NULL, b INT NOT NULL, INDEX a_asc_b_desc (a ASC, b DESC)); 但是在 MySQL 8.0 之前是不支持的，会忽略这个指定。所以在 8.0 之前对降序较多的查询，成本是比较高的。在 8.0 版本之前，可以对降序查询较多的字段做存储时用负值进行存储。 一些使用限制： 降序索引只能在innodb存储引擎中使用，其他存储引擎不支持。 降序索引只支持 BTREE 索引，不支持 HASH 索引。 Web3wtf.academyhttps://wtf.academy/learning-center一个 solidity 学习的网站，每个章节结束之后有练习可以做。","categories":[],"tags":[{"name":"weekly","slug":"weekly","permalink":"https://orechou.github.io/tags/weekly/"}]},{"title":"【读书】《被讨厌的勇气：“自我启发之父”阿德勒的哲学课》","slug":"reading/被讨厌的勇气：“自我启发之父”阿德勒的哲学课","date":"2022-08-13T07:37:00.000Z","updated":"2022-08-13T07:40:45.264Z","comments":true,"path":"2022/08/13/reading/被讨厌的勇气：“自我启发之父”阿德勒的哲学课/","link":"","permalink":"https://orechou.github.io/2022/08/13/reading/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94%EF%BC%9A%E2%80%9C%E8%87%AA%E6%88%91%E5%90%AF%E5%8F%91%E4%B9%8B%E7%88%B6%E2%80%9D%E9%98%BF%E5%BE%B7%E5%8B%92%E7%9A%84%E5%93%B2%E5%AD%A6%E8%AF%BE/","excerpt":"","text":"三句话总结本书 人生目的论。你现在所做所为，所经历、遭遇的一切都不是过去的原因造成的，而是受现在你的目的影响。（过去已经发生，你所能做的唯有改变自己的目的、心态） 课题分离（理清楚课题属于谁）、他者贡献（自己有贡献感即可，不需要关注别人的看法）。 将人生的聚光灯打在当下，过好此时此刻，不要特别在意过去（过去已经发生的无法改变）和未来（当下做的每一件事都影响着未来）。 简单总结和感受书的内容简单 &amp; 直观，以一个青年人与一个哲人的对话的形式展开。内容不多、章节很短，让人看着不累，我也许久没有完整看完过一本书了。更为主要的是最近工作 &amp; 生活压力比较大，外界噪音也比较多，很难有时间能够静下心来审视自己的内心。 最精华的3句书摘 ”即使你逃避人生课题、依赖人生谎言，那也不是因为你沾染了“恶”。这不是一个应该从道德方面来谴责的问题，它只是“勇气”的问题。“原谅自己、与自己和解：有些场合、有些事情、有些关系，我害怕去做、去拖延、去维护，心里有很大的负罪感、对自己失望，认为自己已经不行了。其实认清这是个勇气的问题，然后解决勇气的问题就好了，不要对自己失望很重要。 “有些男人会骂家庭主妇”又不挣钱！“或者”是谁养着你呀？“之类的话，也听到过有人说”钱随便你花，还有什么不满的呀？“之类的话，这都是多么无情的话呀！经济地位跟人的价值毫无关系。公司职员和家庭主妇只是劳动场所和任务不同，完全是”虽不同但平等“。”我已经成家立业，与妻子也即将迎来爱情的结晶。摘录这段问题是提醒自己不要变成这种讨厌、无情的人。 “跨出家门的那一瞬间，“旅行”已经开始，朝着目的地出发途中的每一个瞬间都是旅行。当然，即使因为某些事情而没能够到达金字塔，那也并非没有旅行。这就是现实性的人生。”我一直都在旅途中，希望自己把聚光灯打在当下，过好每一天、做好每一件事，别揪着过去不放，又一直对未来停留在幻想。","categories":[],"tags":[{"name":"Reading","slug":"Reading","permalink":"https://orechou.github.io/tags/Reading/"}]},{"title":"【生活】手机应用安装 & 使用指南","slug":"life/手机应用安装 & 使用指南","date":"2022-06-04T16:20:00.000Z","updated":"2022-06-05T07:48:04.198Z","comments":true,"path":"2022/06/05/life/手机应用安装 & 使用指南/","link":"","permalink":"https://orechou.github.io/2022/06/05/life/%E6%89%8B%E6%9C%BA%E5%BA%94%E7%94%A8%E5%AE%89%E8%A3%85%20&%20%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"如今的世界，手机已经属于出行、工作、娱乐、生活所必不可缺少的设备。各种应用软件，例如知乎、抖音、淘宝等占据了人生的大部分时间。我作为电子产品的重度用户，在每天与电子设备的接触过程中，不仅耗费了大量的时间、也让身体出现一些问题。鉴于此，我决定优化手机应用软件的安装、使用，将手机的娱乐属性剔除，降低一些无效时间的消耗。 原则No.1 卸载所有游戏我不认为能在手机上玩到真正的好游戏，花点钱买 PS5 、Switch 你会有更好的游戏体验。手机游戏的最大优势是能够在任何时候杀掉你的时间。 No.2 劲量使用国外的软件 &amp; 服务如果不想被封禁、被 404，劲量选用国外的软件 &amp; 服务，做一个数字移民。 No.3 软件 &amp; 服务如果能通过电脑访问，劲量减少手机版安装此条规则主要是针对一些非实时性知识软件，例如：知乎、B 站。我在手机上刷知乎和 B 站大部分是娱乐内容、碎片化的讯息，过了一遍大脑什么都不会留下。如果使用电脑，我才会是主动有目的性的去了解一些内容。 No.4 卸载长时间不用的软件，让手机简单长时间不用的软件大概率之后也不会使用，而且现在流量很便宜，先卸载也没用任何关系。 一些软件推荐阅读iBooks目前对于数字书籍，已经弃坑了 kindle 、微信读书等一众硬软件。目前使用 epub 格式电子书 + iBooks + iCloud，可以在 iPhone、iPad 和 mac 上无缝切换，同步阅读记录、进度。 金融投资Futubull雪球Binance","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"https://orechou.github.io/tags/Life/"}]},{"title":"【Java】自定义 Spring Security 鉴权","slug":"java/Java_自定义 Spring Security 鉴权","date":"2022-06-01T09:00:00.000Z","updated":"2022-06-04T16:06:49.912Z","comments":true,"path":"2022/06/01/java/Java_自定义 Spring Security 鉴权/","link":"","permalink":"https://orechou.github.io/2022/06/01/java/Java_%E8%87%AA%E5%AE%9A%E4%B9%89%20Spring%20Security%20%E9%89%B4%E6%9D%83/","excerpt":"","text":"背景当我们使用 Spring 技术栈搭建单体的系统或服务时，若系统涉及到登录鉴权等功能，一般会使用 SpringSecurity 搭配一个 RBAC 的权限模型，很容易实现一套 OAuth2 鉴权、授权的流程。随着业务扩展，单体的服务作为一个微服务并入一个大的系统之后。我们为保证其他业务能够调用该单体服务，但是又无法让其他系统来使用该服务已有鉴权，从而引入其他微服务都用基础的鉴权服务。根据业务场景的区分，当单体服务内部使用的时候走 SpringSecurity 的鉴权，单体服务外部调用的时候使用基础鉴权服务鉴权。 基础要根据不同的场景启用或绕过 SpringSecurity。我们可以首先想到能否通过 URL 来区分不同的场景，使用 HttpSecurity 的 permitAll() 和 authenticated() 来让不同路径的接口是否需要走鉴权。除此之外，可以去修改 SpringSecurity 的 Filter ，使用自定义 Filter 或者用动态代理对 Filter 进行增强。 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Aspect@Component@Slf4jpublic class AuthorizationHeaderAspect &#123; /** * 基础鉴权服务 Feign 调用业务 */ @Autowired private OauthFeignBiz oauthFeignBiz; @Pointcut(&quot;execution(* org.springframework.security.oauth2.provider.authentication.OAuth2AuthenticationProcessingFilter.doFilter(..))&quot;) public void securityOauth2DoFilter() &#123; &#125; @Pointcut(&quot;execution(* org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(..))&quot;) public void securitySecurityInterceptor2DoFilter() &#123; &#125; @Around(&quot;securityOauth2DoFilter()&quot;) public void enhanceSecurityOauth2DoFilter(ProceedingJoinPoint joinPoint) throws Throwable &#123; Object[] args = joinPoint.getArgs(); if (args == null || args.length != 3 || !(args[0] instanceof HttpServletRequest &amp;&amp; args[1] instanceof javax.servlet.ServletResponse &amp;&amp; args[2] instanceof FilterChain)) &#123; joinPoint.proceed(); return; &#125; HttpServletRequest request = (HttpServletRequest) args[0]; String accessToken = request.getHeader(&quot;Authorization&quot;); if (StringUtils.isNotBlank(request.getParameter(&quot;sceneId&quot;)) &amp;&amp; StringUtils.isNotBlank(request.getParameter(&quot;sceneType&quot;))) &#123; // 这里我们可以根据业务场景确定走哪种鉴权方式 SecurityUtils.setBaseAuth(true); &#125; else &#123; SecurityUtils.setBaseAuth(false); &#125; if (SecurityUtils.isBaseAuth()) &#123; Response&lt;CheckTokenResponse&gt; checkTokenResponse = oauthFeignBiz.checkToken(accessToken); if (checkTokenResponse.getCode() == 0) &#123; SecurityUtils.setBaseAuth((true)); ((FilterChain) args[2]).doFilter((ServletRequest) args[0], (ServletResponse) args[1]); &#125; else &#123; throw new Exception(&quot;鉴权失败&quot;); &#125; &#125; else &#123; // 让原 Filter 的逻辑继续执行 joinPoint.proceed(); &#125; &#125; @Around(&quot;securitySecurityInterceptor2DoFilter()&quot;) public void enhanceSecuritySecurityInterceptor2DoFilter(ProceedingJoinPoint joinPoint) throws Throwable &#123; Object[] args = joinPoint.getArgs(); if (args == null || args.length != 3 || !(args[0] instanceof HttpServletRequest &amp;&amp; args[1] instanceof javax.servlet.ServletResponse &amp;&amp; args[2] instanceof FilterChain)) &#123; joinPoint.proceed(); return; &#125; if (!SecurityUtils.isBaseOauth()) &#123; joinPoint.proceed(); return; &#125; ((FilterChain) args[2]).doFilter((ServletRequest) args[0], (ServletResponse) args[1]); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://orechou.github.io/tags/Java/"}]},{"title":"【资源】Batcave Resources 第一期","slug":"other/网络资源","date":"2022-05-13T15:40:00.000Z","updated":"2022-07-12T07:41:33.804Z","comments":true,"path":"2022/05/13/other/网络资源/","link":"","permalink":"https://orechou.github.io/2022/05/13/other/%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90/","excerpt":"","text":"下面记录一些平时收录的一些资源。 资源站[1]Z-Library: 全球最大的数字图书馆目前对于数字书籍，已经弃坑了 kindle 、微信读书等一众硬软件。目前使用 epub 格式电子书 + iBooks + iCloud，可以在 iPhone、iPad 和 mac 上无缝切换，同步阅读记录、进度。 小工具[1]Batman-Logo: 新蝙蝠侠 Logo 生成器 [2]earth.fm: 聆听世界的白噪音 [3]muselink: 极简数字名片，只为创作者 [4]imgg: 以图片的方式截取并分享万物，简单，美观且隐私安全 [5]ActivityWatch: 一款免费的自动追踪在你设备上使用时间的软件","categories":[],"tags":[{"name":"Other","slug":"Other","permalink":"https://orechou.github.io/tags/Other/"}]},{"title":"【歌曲】Deadman's Gun && That's The Way It Is","slug":"life/Deadman's Gun","date":"2022-04-27T16:25:00.000Z","updated":"2022-07-30T09:27:42.092Z","comments":true,"path":"2022/04/28/life/Deadman's Gun/","link":"","permalink":"https://orechou.github.io/2022/04/28/life/Deadman's%20Gun/","excerpt":"","text":"成长道路是比较痛苦的，从青年一路走过来，自己的棱角被磨圆，没有了以往的神气。往往在深夜的时候，想起曾经那些喜欢的英雄，都已过去，都已逝去。而我才后知后觉，忘了告别。 歌曲 Deadman’s Gun 的 YouTube 地址。歌词如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Your hands uponA deadman&#x27;s gun, and you&#x27;reLookin&#x27; down the sightsYour heart is warmAnd the seams are torn, and they&#x27;veGiven you a reason to fightAnd you&#x27;re not gonna takeWhat they&#x27;ve got to giveAnd you&#x27;re not gonna let &#x27;em take your will to liveBecause they&#x27;ve taken enoughAnd you&#x27;ve given them all you can giveAnd luck won&#x27;t save them tonightThey&#x27;ve given you a reason to fightAnd all the storms you&#x27;ve been chasin&#x27;About to rain down tonightAnd all that pain you&#x27;ve been facin&#x27;About to come into the lightYour hands uponA deadman&#x27;s gun, and you&#x27;reLookin&#x27; down the sightsYour heart is warmAnd the seams are torn, and they&#x27;veGiven you a reason to fightAnd you&#x27;re not gonna takeWhat they&#x27;ve got to giveAnd you&#x27;re not gonna let &#x27;em take your will to liveBecause they&#x27;ve taken enoughAnd you&#x27;ve given them all you can giveAnd luck won&#x27;t save them tonightThey&#x27;ve given you a reason to fightAnd all the storms you&#x27;ve been chasin&#x27;About to rain down tonightAnd all that pain you&#x27;ve been facin&#x27;About to come into the light 歌曲 That’s The Way It Is 的 YouTube 地址。歌词如下： 1234567891011121314151617181920212223242526272829303132333435363738394041[Verse 1]The many miles we walkedThe many things we learnedThe building of a shrineOnly just to burnMay the wind be at your backGood fortune touch your handMay the cards lay out a straightAll from your command[Chorus]That&#x27;s the way it isThat’s the way it isThat&#x27;s the way it isThat&#x27;s the way it is[Verse 2]Blue heron leaves the northern skyTrusts the journey to new heightsWhat’s the meaning of the scarIf we don&#x27;t know how to healShould we ever be apartThen how does it feel[Chorus]That&#x27;s the way it isThat&#x27;s the way it isThat&#x27;s the way it isThat&#x27;s the way it is[Bridge]Shine light into darknessShine light into darkness[Chorus]That&#x27;s the way it isThat&#x27;s the way it is (So many miles to walk)That’s the way it is (So many things we’ve got to learn)That&#x27;s the way it is (Oh, that’s the way it is)That&#x27;s the way it is (Oh)That&#x27;s the way it is","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"https://orechou.github.io/tags/Life/"},{"name":"Song","slug":"Song","permalink":"https://orechou.github.io/tags/Song/"}]},{"title":"【生活】EDC 清单","slug":"life/EDC 清单","date":"2022-04-12T15:40:00.000Z","updated":"2022-05-29T09:32:45.464Z","comments":true,"path":"2022/04/12/life/EDC 清单/","link":"","permalink":"https://orechou.github.io/2022/04/12/life/EDC%20%E6%B8%85%E5%8D%95/","excerpt":"","text":"EDC means everyday carry，即是每日通勤携带的物件。之前看过一本书《只过必要生活》，里面有一个建议是将你每天会用到的物品都放到通勤背包中每天携带。这篇博文就是整理我如今每天必用、必带的物品。 数码物品No.1 MacBook Pro 13公司配发的 MacBook Pro (13-inch, 2020)，Intel i7，32G。用途：写代码，写博客。 No.2 iPhone13 Pro Max自用手机，128G。用途：社交，投资，资讯。 No.3 Apple Watch Series 6自用手表，44 毫米款。用途：看时间，记录数据。 非数码物品No.1 新秀丽背包","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"https://orechou.github.io/tags/Life/"}]},{"title":"【iOS】在地图中绘制坐标轨迹","slug":"iOS/iOS_在地图中绘制坐标轨迹","date":"2022-03-27T09:43:00.000Z","updated":"2022-03-27T11:15:57.214Z","comments":true,"path":"2022/03/27/iOS/iOS_在地图中绘制坐标轨迹/","link":"","permalink":"https://orechou.github.io/2022/03/27/iOS/iOS_%E5%9C%A8%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%BB%98%E5%88%B6%E5%9D%90%E6%A0%87%E8%BD%A8%E8%BF%B9/","excerpt":"","text":"一直有想做属于自己的 App 的想法，零零散散地有学习一些内容，现在决定开启一个记录学习过程的系列博文，以此来巩固自己所学到的 iOS 开发知识和激励自己把第一个 App 实现出来。脑海中一直有个做地图类应用的想法，本科做移动轨迹相关的毕业设计。此类应用主要包括地图（Map）、坐标点（Location）、绘制轨迹（Polyline）几个要素。这篇博文将会使用 SwiftUI 在地图上连接坐标点绘制出轨迹。GPS 坐标数据可以在 microsoft 上下载。效果如下：首先创建一个 SwiftUI 工程的项目，项目的文件组织如下：先定义坐标点的数据结构，代码如下： 12345678910import Foundationstruct Location: Hashable, Codable, Identifiable &#123; var id: Int // 经度 var longitude: Double // 纬度 var latitude: Double var isValid: Bool&#125; 坐标数据的 ViewModel，我们将从 location.json 中加载所有的坐标点，代码如下： 1234567891011121314151617181920212223242526272829import Foundationfinal class LocationViewModel: ObservableObject &#123; @Published var locations: [Location] = load(&quot;location.json&quot;) &#125;func load&lt;T: Decodable&gt;(_ fileName: String) -&gt; T &#123; // The data value type allows simple byte buffers to take on behavior of Foundation objects. let data: Data guard let file = Bundle.main.url(forResource: fileName, withExtension: nil) else &#123; fatalError(&quot;Couldn&#x27;t find \\(fileName) in main bundle.&quot;) &#125; do &#123; data = try Data(contentsOf: file) &#125; catch &#123; fatalError(&quot;Couldn&#x27;t load \\(fileName) from main bundle:\\n\\(error)&quot;) &#125; do &#123; let decoder = JSONDecoder() return try decoder.decode(T.self, from: data) &#125; catch &#123; fatalError(&quot;Couldn&#x27;t parse \\(fileName) as \\(T.self):\\n\\(error)&quot;) &#125;&#125; 接下来我们将利用 UIViewRepresentable 这个协议实现将 UIKit 的 UIView 集成到 SwiftUI 中使用。UIViewRepresentable 只包含四个方法。其中 makeUIView , updateUIView 用于创建和更新视图。makeCoordinator 创建协调器，用于通知其它 UI 与该 View 之间的变化更新。dismantleUIView 用于在移除 View 时做一些善后操作。四个方法在 UIViewRepresentable 生命周期中的调用顺序如下：我们的 MapView 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import SwiftUIimport MapKitstruct MapView: UIViewRepresentable &#123; private let locationViewModel = LocationViewModel() private let mapZoomEdgeInsets = UIEdgeInsets(top: 30.0, left: 30.0, bottom: 30.0, right: 30.0) func makeCoordinator() -&gt; MapViewCoordinator &#123; MapViewCoordinator(self) &#125; func makeUIView(context: Context) -&gt; MKMapView &#123; let mapView = MKMapView() mapView.showsUserLocation = true mapView.delegate = context.coordinator return mapView &#125; func updateUIView(_ uiView: MKMapView, context: UIViewRepresentableContext&lt;MapView&gt;) &#123; updateOverlays(from: uiView) &#125; private func updateOverlays(from mapView: MKMapView) &#123; mapView.removeOverlays(mapView.overlays) var coordinates = [CLLocationCoordinate2D]() for i in 0..&lt;locationViewModel.locations.count &#123; let location = locationViewModel.locations[i] coordinates.append(CLLocationCoordinate2D(latitude: location.latitude, longitude: location.longitude)) &#125; let polyline = MKPolyline(coordinates: coordinates, count: locationViewModel.locations.count) mapView.addOverlay(polyline) setMapZoomArea(map: mapView, polyline: polyline, edgeInsets: mapZoomEdgeInsets, animated: true) &#125; private func setMapZoomArea(map: MKMapView, polyline: MKPolyline, edgeInsets: UIEdgeInsets, animated: Bool = false) &#123; map.setVisibleMapRect(polyline.boundingMapRect, edgePadding: edgeInsets, animated: animated) &#125;&#125;struct MapView_Previews: PreviewProvider &#123; static var previews: some View &#123; MapView() &#125;&#125; 协调器 MayViewCoordinator 代码如下： 12345678910111213141516171819202122232425262728import Foundationimport MapKitfinal class MapViewCoordinator: NSObject, MKMapViewDelegate &#123; private let mapView: MapView init(_ control: MapView) &#123; self.mapView = control &#125; func mapView(_ mapView: MKMapView, didAdd views: [MKAnnotationView]) &#123; if let annotationView = views.first, let annotation = annotationView.annotation &#123; if annotation is MKUserLocation &#123; let region = MKCoordinateRegion(center: annotation.coordinate, latitudinalMeters: 1000, longitudinalMeters: 1000) mapView.setRegion(region, animated: true) &#125; &#125; &#125; func mapView(_ mapView: MKMapView, rendererFor overlay: MKOverlay) -&gt; MKOverlayRenderer &#123; let renderer = MKPolylineRenderer(overlay: overlay) // set the line properties such as color and width renderer.strokeColor = .blue renderer.lineWidth = 3.0 return renderer &#125;&#125; 参考链接https://www.fatbobman.com/posts/uikitInSwiftUI/https://medium.com/@mauvazquez/decoding-a-polyline-and-drawing-it-with-swiftui-mapkit-611952bd0ecb","categories":[],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://orechou.github.io/tags/iOS/"},{"name":"SwiftUI","slug":"SwiftUI","permalink":"https://orechou.github.io/tags/SwiftUI/"}]},{"title":"【生活】记事一则","slug":"life/记事一则","date":"2022-02-13T07:40:00.000Z","updated":"2022-05-13T15:52:31.736Z","comments":true,"path":"2022/02/13/life/记事一则/","link":"","permalink":"https://orechou.github.io/2022/02/13/life/%E8%AE%B0%E4%BA%8B%E4%B8%80%E5%88%99/","excerpt":"","text":"下午，yoyo 和两支小🐱虎在😴。我坐在客厅写一些文字，记录一下心情。 首先要说的是，年前我和 yoyo 在老家举办完了婚礼，虽然我们还没有领证，但终于我们也算是完成了去年最大、最重要的一个 OKR。现在 yoyo 也🤰🏻了，虽然这也是之前一直计划的事情，但当 ta 真的来了的事情，我内心除了惊喜 max，还是有些慌张的。这种慌张不能让 yoyo 看出来。年后没几天我跟 yoyo 就工作城市深圳了，这座城市是很忙碌、劳累的。所以这段时间，我也一直在考虑怎么让我们更加轻松一些。所以现在关注了本身工作之外的一些内容，例如 区块链、NFTs、美股等等，想要找到一些虽不能说完成跨越，但至少能让自己工作不那么累的事情。我也一直在考虑未来自由职业的道路，但无奈自己现在很缺乏自信和对自己的认同。 今年定了几个 OKR，努力去实现吧。另外我得放松体验生活。不管再多事情，生活是我们自己的。先不说这么多了，我得叫 yoyo 起来出去散步了。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"https://orechou.github.io/tags/Life/"}]},{"title":"【MySQL】实现列表数据置顶","slug":"mysql/MySQL_实现列表数据置顶","date":"2021-08-16T06:46:00.000Z","updated":"2022-03-27T11:14:09.194Z","comments":true,"path":"2021/08/16/mysql/MySQL_实现列表数据置顶/","link":"","permalink":"https://orechou.github.io/2021/08/16/mysql/MySQL_%E5%AE%9E%E7%8E%B0%E5%88%97%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%BD%AE%E9%A1%B6/","excerpt":"","text":"有的列表获取的业务场景，需要根据一些条件将数据置顶。 例如有一张表有 id 和 name 两个字段。数据表如下。 id name 1 1_name 2 2_name 3 3_name 4 4_name 5 5_name 6 6_name 7 7_name 8 8_name 9 9_name 10 10_name 若需要将 id 为 5 和 7 的数据置顶，并且分页的 size 是 5 的话，我们的 SQL 可以这样写： 1234567SELECT * FROM goods ORDER BY CASE WHEN id = 5 THEN -10000 WHEN id = 7 THEN -9999 ELSE id ENDLIMIT 0, 5","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://orechou.github.io/tags/MySQL/"}]},{"title":"【MySQL】慢 SQL 分析","slug":"mysql/MySQL_慢 SQL 分析","date":"2021-08-10T04:30:00.000Z","updated":"2022-03-27T11:14:03.743Z","comments":true,"path":"2021/08/10/mysql/MySQL_慢 SQL 分析/","link":"","permalink":"https://orechou.github.io/2021/08/10/mysql/MySQL_%E6%85%A2%20SQL%20%E5%88%86%E6%9E%90/","excerpt":"","text":"慢查询日志分析设置慢查询可以通过修改命令设置： 设置开启：SET GLOBAL slow_query_log = 1; #默认未开启，开启会影响性能，mysql重启会失效 查看是否开启：SHOW VARIABLES LIKE ‘%slow_query_log%’; 设置阈值：SET GLOBAL long_query_time=3; 查看阈值：SHOW 【GLOBAL】 VARIABLES LIKE ‘long_query_time%’; #重连或新开一个会话才能看到修改值 也可以通过修改配置文件设置，配置文件 my.conf 会一直生效，在[mysqld]下配置： 12345[mysqld]slow_query_log = 1; #开启slow_query_log_file=/var/lib/mysql/atguigu-slow.log #慢日志地址，缺省文件名host_name-slow.loglong_query_time=3; #运行时间超过该值的SQL会被记录，默认值&gt;10log_output=FILE 获取慢 SQL 信息查看慢查询日志记录数：SHOW GLOBAL STATUS LIKE ‘%Slow_queries%’; 模拟语句：SELECT SLEEP(4); 查看慢查询日志： 12345678910$ cat /usr/local/var/mysql/OreChoudeMacBook-Pro-slow.log/usr/local/Cellar/mysql/8.0.25_1/bin/mysqld, Version: 8.0.25 (Homebrew). started with:Tcp port: 3306 Unix socket: /tmp/mysql.sockTime Id Command Argument# Time: 2021-08-10T06:29:53.513752Z# User@Host: root[root] @ localhost [127.0.0.1] Id: 11# Query_time: 4.003605 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 1use test;SET timestamp=1628576989;select sleep(4); 使用 mysqldumpslow 分析使用样例： mysqldumpslow -s r -t 10 /usr/local/var/mysql/OreChoudeMacBook-Pro-slow.log #得到返回记录集最多的10个SQL mysqldumpslow -s c -t 10 /usr/local/var/mysql/OreChoudeMacBook-Pro-slow.log #得到访问次数最多的10个SQL mysqldumpslow -s t -t 10 -g “LEFT JOIN” /usr/local/var/mysql/OreChoudeMacBook-Pro-slow.log #得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s r -t 10 /usr/local/var/mysql/OreChoudeMacBook-Pro-slow.log | more #结合| more使用，防止爆屏情况 Explain 分析Show Profile 分析Show Profile 能够获取比 Explain 更为详细的信息，能够分析当前会话中语句执行时的资源消耗，获取 SQL 在整个生命周期的时间。 开启 Profile12开启：set profiling = on;查看：SHOW VARIABLES LIKE &#x27;profiling%&#x27;; 开启后 MySQL 后台会保存最近 15 次的结果。 查看 Profile使用 SHOW PROFILES 可以查看最近的 15 次结果。 查看具体的 Profile通过 Query_ID 可以得到具体 SQL 从连接 - 服务 - 引擎 - 存储四层结构完整生命周期的耗时。 使用命令：SHOW PROFILE CPU, BLOCK IO FOR Query_ID 可用参数type: ALL #显示所有的开销信息 BLOCK IO #显示块IO相关开销 CONTEXT SWITCHES #上下文切换相关开销 CPU #显示CPU相关开销信息 IPC #显示发送和接收相关开销信息 MEMORY #显示内存相关开销信息 PAGE FAULTS #显示页面错误相关开销信息 SOURCE #显示和Source_function，Source_file，Source_line相关的开销信息 SWAPS #显示交换次数相关开销的信息 如果出现以下几个状态则 SQL 需要重点分析： converting HEAP to MyISAM #查询结果太大，内存不够用了，在往磁盘上搬 Creating tmp table #创建了临时表，回先把数据拷贝到临时表，用完后再删除临时表 Copying to tmp table on disk #把内存中临时表复制到磁盘，危险！！！ locked","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://orechou.github.io/tags/MySQL/"}]},{"title":"【MySQL】Explain","slug":"mysql/MySQL_Explain","date":"2021-08-06T09:00:00.000Z","updated":"2022-03-27T11:14:13.632Z","comments":true,"path":"2021/08/06/mysql/MySQL_Explain/","link":"","permalink":"https://orechou.github.io/2021/08/06/mysql/MySQL_Explain/","excerpt":"","text":"MySQL ExplainExplain 是 MySQL 自带的查询优化器。 使用 Explain + SQL 可查询出执行的相关信息，主要包含以下 10 个属性：id, select_type, table, type, possible_key, key, key_len, ref, row, filtered, Extra 数据库性能瓶颈，主要关注 CPU 和 IO。 id反映的是表的读取顺序，或者查询中 SELECT 的执行顺序。 小表永远驱动大表，三种情况：（1）id 相同，执行顺序是由上至下的（2）id 不同，如果是子查询，id 序号会递增，id 值越大优先级越高，越先被执行（3）id 存在相同的，也存在不同的，所有组中，id 越大越先执行，如果 id 相同的，从上往下顺序执行 select_type反映的是 MySQL 理解的查询类型，有几下几种： SIMPLE：简单的 SELECT 查询，查询中不包含子查询或 UNION PRIMARY：查询中若包含任何复杂的字部分，最外层查询标记为 PRIMARY SUBQUERY：SELECT 或 WHERE 列表中的子查询 DERIVED：在 FROM 列表中包含的子查询，MySQL 会递归执行这些子查询，把结果放在临时表里 UNION：若第二个 SELECT 出现在 UNION 后，则被标记为 UNION，若 UNION 包含在 FROM 字句的子查询中，外层 SELECT 将被标记为 DERIVED UNION RESULT：UNION 后的结果集 table反映的是数据从哪张表中读取出来。 例如 &lt;derived2&gt; 表示从 id 为 2 的临时表读取。 typetype 是访问类型排序，反映的是 SQL 的优化状态，有如下几种： system：从单表只查出一行记录（等于系统表），这是 const 类型的特例，一般不会出现 const：查询条件用到了常量，通过索引一次就找到，常在使用 primary key 或 unique 索引中出现 eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描 ref：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，它可能会找到多个符合条件的行，与eq_ref的差别是eq_ref只匹配了一条记录 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般是在where语句中出现了 between、&lt;、&gt;、in 等的查询 index：full Index scan，index 和 all 的区别为 index 类型只遍历索引树。这通常比 all 快，因为索引文件通常比数据文件小 all：全表扫描，如果查询数据量很大时，全表扫描效率是很低的 在 SQL 优化中至少做到 range 级别，最好能达到 ref 级别 possible_key &amp; key &amp; key_lenpossible_key 反映的是 MySQL 推测可能用到的索引，不一定被查询实际使用到。key 反映的是实际使用到的索引，若为 null 则是因为没有建索引或者索引失效。key_len 反映索引中使用的字节数，可计算计算查询中使用的索引的长度，越短越好。其显示的值为索引字段的最大可能长度，而非实际使用长度。 refref 反映的是哪些列或者常量被用于查找索引列上的值。 rowsrows 反映的根据表的统计信息和索引选用的情况，大致估算出来到找到所有记录所需要读取的行数。 filtered使用 explain extended 时会出现这个列，5.7 之后的版本默认就有这个字段。这个字段表示存储引擎返回的数据在 server 层过滤后，剩下多少满足查询的记录数量的比例，不是具体记录数。 ExtraExtra 反映的不适合在其他列显示，但是也很重要的信息，主要有以下几种： Using filesort：MySQL 中无法利用索引完成的排序，这时会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。 Using temporary：使用了临时表保存中间结果，MySQL 在对查询结果排序时使用临时表。常见于排序 order by 和分组查询 group by。 Using index：MySQL 相应的 select 操作中使用了覆盖索引，避免了访问表的数据行，效率高。 Using where：MySQL 使用了 where 过滤。 Using join buffer：MySQL 使用了连接缓存。 Impossible where：where 子句的值为 false。 Distinct：优化 distinct 操作，在找到第一匹配的元组后即停止找同样值的动作。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://orechou.github.io/tags/MySQL/"}]}],"categories":[],"tags":[{"name":"weekly","slug":"weekly","permalink":"https://orechou.github.io/tags/weekly/"},{"name":"Reading","slug":"Reading","permalink":"https://orechou.github.io/tags/Reading/"},{"name":"Life","slug":"Life","permalink":"https://orechou.github.io/tags/Life/"},{"name":"Java","slug":"Java","permalink":"https://orechou.github.io/tags/Java/"},{"name":"Other","slug":"Other","permalink":"https://orechou.github.io/tags/Other/"},{"name":"Song","slug":"Song","permalink":"https://orechou.github.io/tags/Song/"},{"name":"iOS","slug":"iOS","permalink":"https://orechou.github.io/tags/iOS/"},{"name":"SwiftUI","slug":"SwiftUI","permalink":"https://orechou.github.io/tags/SwiftUI/"},{"name":"MySQL","slug":"MySQL","permalink":"https://orechou.github.io/tags/MySQL/"}]}